{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9402aacf-268e-4fb8-9e6f-ed18a40c4330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, GRU, Input, Multiply, TimeDistributed, Softmax, Lambda, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from xgboost import XGBClassifier\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "        \n",
    "\n",
    "def print_results(outcome_variable):\n",
    "        ### Deep Learning analysis data preparation\n",
    "        # Load data from a CSV file\n",
    "        df = pd.read_csv('D:/Users/yipeng_wei/Documents/dl data/2025-04-18/df_dl_continous.csv')\n",
    "        df_mask = pd.read_csv('D:/Users/yipeng_wei/Documents/dl data/2025-04-18/df_dl_continous_mask.csv')\n",
    "        df_delta = pd.read_csv('D:/Users/yipeng_wei/Documents/dl data/2025-04-18/df_dl_continous_delta.csv')\n",
    "        \n",
    "        # Drop rows with NaN in the target column before any operations\n",
    "        df = df.dropna(subset=[outcome_variable])\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "        df_mask = df_mask.dropna(subset=[outcome_variable])\n",
    "        df_mask = df_mask.reset_index(drop=True)\n",
    "        \n",
    "        df_delta = df_delta.dropna(subset=[outcome_variable])\n",
    "        df_delta = df_delta.reset_index(drop=True)\n",
    "        \n",
    "        features_categorical = [\"TYPE_VISIT\",\"SITE\",\n",
    "                    \"STILLBIRTH_IND\",\"PRETERM_IND\",\"CESARIAN_IND\",\"NUM_MISCARRIAGE_ind\",\n",
    "                    \"DEPR_EVER\",\n",
    "                    \"PAID_WORK\",\"GPARITY_2\",\"GPARITY_1\",\"WEALTH_QUINT_1\",\"WEALTH_QUINT_2\",\"WEALTH_QUINT_3\",\"WEALTH_QUINT_4\",\"SCHOOL_MORE10\",\"WATER_IMPROVED\",\"TOILET_IMPROVED\",\"STOVE_FUEL\",\"HH_SMOKE\",\"SMOKE\",\"CHEW_TOBACCO\",\"CHEW_BETELNUT\",\n",
    "                    \"BMI_LEVEL_ENROLL_underweight\",\"BMI_LEVEL_ENROLL_overweight\",\"BMI_LEVEL_ENROLL_obese\",\n",
    "                    \"GWG_ADEQUACY_inadequate\",\"GWG_ADEQUACY_excessive\",\n",
    "                    \"MAT_CES_ANY\",\"BIRTH_FACILITY\",\"MAT_PRETERM_ANY\",\n",
    "                    \"HTN\",\"DIAB_OVERT_ANY\",\"HIV_POSITIVE_ENROLL\",\"TB_SYMP_POSITIVE_ENROLL\",\"MAL_POSITIVE_ENROLL\",\"STI_POSITIVE_ENROLL\",\n",
    "                    \"GES_HTN\",\"DIAB_GEST_ANY\",\"HIV_POSITIVE_ANY_ANC\",\"TB_SYMP_POSITIVE_ANY_ANC\",\"MAL_POSITIVE_ANY_ANC\",\"HBV_POSITIVE_ANY_ANC\",\"HCV_POSITIVE_ANY_ANC\",\"STI_POSITIVE_ANY_ANC\",\n",
    "                    \"INF_SEX\",\"INF_HYPERBILI_NICE\",\"BREASTFED\",\n",
    "                    \"NUM_FETUS_2\",\"NUM_FETUS_3\",\"INF_ANOMALY\",\"PREVIA\",\n",
    "                    \"INF_PSBI_IPC\"]\n",
    "        \n",
    "        features_continous = [\"AGE_GROUP\",\"MAT_AGE\",\"MUAC_ENROLL\",\"BMI_ENROLL\",\"NUM_FETUS\",\"NUM_MISCARRIAGE\",\"GESTAGEBIRTH_ANY\",\"BWEIGHT_ANY\",\"CROWDING_IND\",\n",
    "                              \"M01_AFI_PERES\",\n",
    "                              \"M08_IRON_TOT_UGDL_LBORRES\", \"M08_FERRITIN_LBORRES\",\"M08_IRON_HEP_LBORRES\"]\n",
    "        \n",
    "        features_static = [\"SITE\",\n",
    "                    \"STILLBIRTH_IND\",\"PRETERM_IND\",\"CESARIAN_IND\",\"NUM_MISCARRIAGE_ind\",\n",
    "                    \"DEPR_EVER\",\n",
    "                    \"PAID_WORK\",\"GPARITY_2\",\"GPARITY_1\",\"WEALTH_QUINT_1\",\"WEALTH_QUINT_2\",\"WEALTH_QUINT_3\",\"WEALTH_QUINT_4\",\"SCHOOL_MORE10\",\"WATER_IMPROVED\",\"TOILET_IMPROVED\",\"STOVE_FUEL\",\"HH_SMOKE\",\"SMOKE\",\"CHEW_TOBACCO\",\"CHEW_BETELNUT\",\"CROWDING_IND\",\n",
    "                    \"BMI_LEVEL_ENROLL_underweight\",\"BMI_LEVEL_ENROLL_overweight\",\"BMI_LEVEL_ENROLL_obese\",\n",
    "                    \"GWG_ADEQUACY_inadequate\",\"GWG_ADEQUACY_excessive\",\n",
    "                    \"MAT_CES_ANY\",\"BIRTH_FACILITY\",\"MAT_PRETERM_ANY\",\n",
    "                    \"HTN\",\"DIAB_OVERT_ANY\",\"HIV_POSITIVE_ENROLL\",\"TB_SYMP_POSITIVE_ENROLL\",\"MAL_POSITIVE_ENROLL\",\"STI_POSITIVE_ENROLL\",\n",
    "                    \"GES_HTN\",\"DIAB_GEST_ANY\",\"HIV_POSITIVE_ANY_ANC\",\"TB_SYMP_POSITIVE_ANY_ANC\",\"MAL_POSITIVE_ANY_ANC\",\"HBV_POSITIVE_ANY_ANC\",\"HCV_POSITIVE_ANY_ANC\",\"STI_POSITIVE_ANY_ANC\",\n",
    "                    \"INF_SEX\",\"INF_HYPERBILI_NICE\",\"BREASTFED\",\"GESTAGEBIRTH_ANY\",\"BWEIGHT_ANY\",\n",
    "                    \"INF_ANOMALY\",\n",
    "                    \"INF_PSBI_IPC\",\n",
    "                    \"NUM_FETUS_2\",\"NUM_FETUS_3\",\"MAT_AGE\",\"MUAC_ENROLL\",\n",
    "                    \"AGE_GROUP\",\"BMI_ENROLL\",\"NUM_FETUS\",\"NUM_MISCARRIAGE\"]\n",
    "        \n",
    "        features_temporal = [\"TYPE_VISIT\",\n",
    "                             \"M01_AFI_PERES\",\n",
    "                             \"PREVIA\",\n",
    "                             \"M08_IRON_TOT_UGDL_LBORRES\",\"M08_FERRITIN_LBORRES\",\"M08_IRON_HEP_LBORRES\"]\n",
    "\n",
    "        features_ml = [\"SITE\",\n",
    "                    \"STILLBIRTH_IND\",\"PRETERM_IND\",\"CESARIAN_IND\",\"NUM_MISCARRIAGE_ind\",\n",
    "                    \"DEPR_EVER\",\n",
    "                    \"PAID_WORK\",\"GPARITY_2\",\"GPARITY_1\",\"WEALTH_QUINT_1\",\"WEALTH_QUINT_2\",\"WEALTH_QUINT_3\",\"WEALTH_QUINT_4\",\"SCHOOL_MORE10\",\"WATER_IMPROVED\",\"TOILET_IMPROVED\",\"STOVE_FUEL\",\"HH_SMOKE\",\"SMOKE\",\"CHEW_TOBACCO\",\"CHEW_BETELNUT\",\"CROWDING_IND\",\n",
    "                    \"BMI_LEVEL_ENROLL_underweight\",\"BMI_LEVEL_ENROLL_overweight\",\"BMI_LEVEL_ENROLL_obese\",\n",
    "                    \"GWG_ADEQUACY_inadequate\",\"GWG_ADEQUACY_excessive\",\n",
    "                    \"MAT_CES_ANY\",\"BIRTH_FACILITY\",\"MAT_PRETERM_ANY\",\n",
    "                    \"HTN\",\"DIAB_OVERT_ANY\",\"HIV_POSITIVE_ENROLL\",\"TB_SYMP_POSITIVE_ENROLL\",\"MAL_POSITIVE_ENROLL\",\"STI_POSITIVE_ENROLL\",\n",
    "                    \"GES_HTN\",\"DIAB_GEST_ANY\",\"HIV_POSITIVE_ANY_ANC\",\"TB_SYMP_POSITIVE_ANY_ANC\",\"MAL_POSITIVE_ANY_ANC\",\"HBV_POSITIVE_ANY_ANC\",\"HCV_POSITIVE_ANY_ANC\",\"STI_POSITIVE_ANY_ANC\",\n",
    "                    \"INF_SEX\",\"INF_HYPERBILI_NICE\",\"BREASTFED\",\"GESTAGEBIRTH_ANY\",\"BWEIGHT_ANY\",\n",
    "                    \"INF_ANOMALY\",\n",
    "                    \"INF_PSBI_IPC\",\n",
    "                    \"NUM_FETUS_2\",\"NUM_FETUS_3\",\"MAT_AGE\",\"MUAC_ENROLL\",\n",
    "                    \"AGE_GROUP\",\"BMI_ENROLL\",\"NUM_FETUS\",\"NUM_MISCARRIAGE\",\n",
    "                    \"M01_AFI_PERES_1\",\"M01_AFI_PERES_2\",\"M01_AFI_PERES_3\",\"M01_AFI_PERES_4\",\"M01_AFI_PERES_5\",\n",
    "                    \"PREVIA_1\",\"PREVIA_2\",\"PREVIA_3\",\"PREVIA_4\",\"PREVIA_5\",\n",
    "                    \"M08_FERRITIN_LBORRES_1\", \"M08_FERRITIN_LBORRES_2\", \"M08_FERRITIN_LBORRES_3\", \"M08_FERRITIN_LBORRES_4\", \"M08_FERRITIN_LBORRES_5\",\n",
    "                    \"M08_IRON_TOT_UGDL_LBORRES_1\", \"M08_IRON_TOT_UGDL_LBORRES_2\", \"M08_IRON_TOT_UGDL_LBORRES_3\", \"M08_IRON_TOT_UGDL_LBORRES_4\", \"M08_IRON_TOT_UGDL_LBORRES_5\",\n",
    "                    \"M08_IRON_HEP_LBORRES_1\", \"M08_IRON_HEP_LBORRES_2\", \"M08_IRON_HEP_LBORRES_3\", \"M08_IRON_HEP_LBORRES_4\", \"M08_IRON_HEP_LBORRES_5\"]\n",
    "\n",
    "        features_to_remove_by_outcome = {\n",
    "        \"STILLBIRTH_SIGNS_LIFE\": [\"INF_SEX\",\"INF_HYPERBILI_NICE\",\"BREASTFED\",\"GESTAGEBIRTH_ANY\",\"MAT_PRETERM_ANY\",\"BWEIGHT_ANY\",\"INF_PSBI_IPC\"],\n",
    "        \"PRETERMBIRTH_LT37\": [\"INF_HYPERBILI_NICE\",\"BREASTFED\",\"GESTAGEBIRTH_ANY\",\"MAT_PRETERM_ANY\",\"BWEIGHT_ANY\",\"INF_PSBI_IPC\",\"LABOR_ANY\",\"PRO_LABOR\",\"OBS_LABOR\",\"MAT_CES_ANY\",\"BIRTH_FACILITY\"],\n",
    "        \"LBW2500_ANY\": [\"INF_HYPERBILI_NICE\",\"BREASTFED\",\"BWEIGHT_ANY\",\"INF_PSBI_IPC\",\"LABOR_ANY\",\"PRO_LABOR\",\"OBS_LABOR\",\"MAT_CES_ANY\",\"BIRTH_FACILITY\"],\n",
    "        \"INF_PSBI_IPC\": [\"INF_HYPERBILI_NICE\",\"INF_PSBI_IPC\"],\n",
    "        \"SVN\": [\"INF_SEX\",\"INF_HYPERBILI_NICE\",\"GESTAGEBIRTH_ANY\",\"MAT_PRETERM_ANY\",\"BWEIGHT_ANY\",\"INF_PSBI_IPC\",\"BREASTFED\",\"LABOR_ANY\",\"PRO_LABOR\",\"OBS_LABOR\",\"MAT_CES_ANY\",\"BIRTH_FACILITY\"],\n",
    "        \"NEARMISS\": [\"INF_SEX\",\"INF_HYPERBILI_NICE\",\"BREASTFED\",\"GESTAGEBIRTH_ANY\",\"MAT_PRETERM_ANY\",\"BWEIGHT_ANY\",\"INF_PSBI_IPC\"]\n",
    "        }\n",
    "    \n",
    "        features_to_remove = features_to_remove_by_outcome.get(outcome_variable, [])\n",
    "        \n",
    "        # Remove features from the lists\n",
    "        features_categorical = [f for f in features_categorical if f not in features_to_remove]\n",
    "        features_continous = [f for f in features_continous if f not in features_to_remove]\n",
    "        features_static = [f for f in features_static if f not in features_to_remove]\n",
    "        features_temporal = [f for f in features_temporal if f not in features_to_remove]\n",
    "        features_ml = [f for f in features_ml if f not in features_to_remove]\n",
    "        \n",
    "        # Separate categorical and continuous features\n",
    "        df_categorical = df[features_categorical]\n",
    "        df_continuous = df[features_continous]\n",
    "        \n",
    "        # Rescale continuous features to (0, 1) range\n",
    "        scaler = MinMaxScaler()\n",
    "        df_continuous_scaled = pd.DataFrame(scaler.fit_transform(df_continuous), columns=features_continous)\n",
    "        \n",
    "        # Concatenate scaled continuous features with categorical features\n",
    "        X = pd.concat([df_categorical, df_continuous_scaled], axis=1)\n",
    "\n",
    "        ##Static and temporal data\n",
    "        X_static = X[features_static]\n",
    "        \n",
    "        X_temporal = X[features_temporal]\n",
    "        X_temporal_delta = df_delta[features_temporal]\n",
    "        X_temporal_mask = df_mask[features_temporal]\n",
    "        \n",
    "        y = df[outcome_variable]\n",
    "        threshold = (df[outcome_variable] == 1).mean()\n",
    "        \n",
    "        # One-hot encoding for non-binary categorical features in X\n",
    "        X = pd.get_dummies(X, columns=['TYPE_VISIT','SITE'], dummy_na=False)\n",
    "        X_static = pd.get_dummies(X_static, columns=['SITE'], dummy_na=False)\n",
    "        X_temporal = pd.get_dummies(X_temporal, columns=['TYPE_VISIT'], dummy_na=False)\n",
    "        X_temporal_delta = pd.get_dummies(X_temporal_delta, columns=['TYPE_VISIT'], dummy_na=False)\n",
    "        X_temporal_mask = pd.get_dummies(X_temporal_mask, columns=['TYPE_VISIT'], dummy_na=False)\n",
    "        \n",
    "        # Preprocess for Keras models (3D matrix)\n",
    "        n_samples = X_temporal.shape[0] // 5 \n",
    "        X_static_keras = X_static.iloc[::5].values.astype('float32')\n",
    "        \n",
    "        X_temporal_keras = X_temporal.values.reshape(n_samples, 5, X_temporal.shape[1]).astype('float32')\n",
    "        X_temporal_delta_keras = X_temporal_delta.values.reshape(n_samples, 5, X_temporal_delta.shape[1]).astype('float32')\n",
    "        X_temporal_mask_keras = X_temporal_mask.values.reshape(n_samples, 5, X_temporal_mask.shape[1]).astype('float32')\n",
    "        \n",
    "        X_keras = X.values.reshape(n_samples, 5, X.shape[1]).astype('float32')\n",
    "        \n",
    "        # Align y for Keras models\n",
    "        y_keras = y.iloc[::5].values.astype('float32')\n",
    "        \n",
    "        ### Machine Learning analysis data preparation\n",
    "        # Load data from a CSV file\n",
    "        ml = pd.read_csv('D:/Users/yipeng_wei/Documents/dl data/2025-04-18/df_dl_continous_wide.csv')\n",
    "        \n",
    "        # Drop rows with NaN in the target column before any operations\n",
    "        ml = ml.dropna(subset=[outcome_variable])\n",
    "        ml = ml.reset_index(drop=True)\n",
    "        threshold = (ml[outcome_variable] == 1).mean()\n",
    "        \n",
    "        # Normalize specified features before imputing missing values\n",
    "        features_to_normalize = [\"AGE_GROUP\",\"MAT_AGE\",\"MUAC_ENROLL\",\"GESTAGEBIRTH_ANY\",\"BWEIGHT_ANY\",\"CROWDING_IND\",\n",
    "                                 \"BMI_ENROLL\",\"NUM_FETUS\",\"NUM_MISCARRIAGE\",\n",
    "                                 \"M01_AFI_PERES_1\",\"M01_AFI_PERES_2\",\"M01_AFI_PERES_3\",\"M01_AFI_PERES_4\",\"M01_AFI_PERES_5\",\n",
    "                                 \"M08_FERRITIN_LBORRES_1\", \"M08_FERRITIN_LBORRES_2\", \"M08_FERRITIN_LBORRES_3\", \"M08_FERRITIN_LBORRES_4\", \"M08_FERRITIN_LBORRES_5\",\n",
    "                                 \"M08_IRON_TOT_UGDL_LBORRES_1\", \"M08_IRON_TOT_UGDL_LBORRES_2\", \"M08_IRON_TOT_UGDL_LBORRES_3\", \"M08_IRON_TOT_UGDL_LBORRES_4\", \"M08_IRON_TOT_UGDL_LBORRES_5\",\n",
    "                                 \"M08_IRON_HEP_LBORRES_1\", \"M08_IRON_HEP_LBORRES_2\", \"M08_IRON_HEP_LBORRES_3\", \"M08_IRON_HEP_LBORRES_4\", \"M08_IRON_HEP_LBORRES_5\"]\n",
    "        \n",
    "        # Create a MinMaxScaler instance\n",
    "        scaler = MinMaxScaler()\n",
    "        \n",
    "        ml[features_to_normalize] = scaler.fit_transform(ml[features_to_normalize])\n",
    "        \n",
    "        X_ml = ml[features_ml]\n",
    "        y_ml = ml[outcome_variable]\n",
    "        \n",
    "        X_ml = X_ml.fillna(0)\n",
    "        X_ml = pd.get_dummies(X_ml, columns=['SITE'], dummy_na=False)\n",
    "        \n",
    "        # Preprocess for scikit-learn models (2D matrix)\n",
    "        X_sklearn = X_ml.values.astype('float32')\n",
    "        y_sklearn = y_ml.values.astype('float32')\n",
    "        \n",
    "        def evaluate_model(model_func, X, y, model_type=\"sklearn\", threshold=threshold):\n",
    "            # 5-fold cross-validation\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            \n",
    "            # Lists to store scores for each fold\n",
    "            precision_scores = []\n",
    "            recall_scores = []\n",
    "            f1_scores = []  # List to store F1 scores\n",
    "            roc_auc_scores = []  # List to store ROC-AUC scores\n",
    "            \n",
    "            # Cross-validation loop\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                \n",
    "                if model_type == \"sklearn\":\n",
    "                    # Split training data into 90% training and 10% validation\n",
    "                    train_size = int(0.9 * len(X_train))\n",
    "                    X_train_part, X_val = X_train[:train_size], X_train[train_size:]\n",
    "                    y_train_part, y_val = y_train[:train_size], y_train[train_size:]\n",
    "                    \n",
    "                    # scikit-learn model: Logistic Regression, Random Forest, SVM\n",
    "                    model = model_func()\n",
    "                    model.fit(X_train_part, y_train_part)  # Train on 90% of the training data\n",
    "        \n",
    "                    y_val_pred_prob = model.predict_proba(X_val)[:, 1]  # Predicted probabilities for positive class\n",
    "                    y_pred_prob = model.predict_proba(X_test)[:, 1]  # Predicted probabilities for positive class:\n",
    "                \n",
    "                else:  # Keras models: GRU, RETAIN, PredictPTB\n",
    "                    X_train_part, X_val = X_train[:int(0.9 * len(X_train))], X_train[int(0.9 * len(X_train)):]\n",
    "                    y_train_part, y_val = y_train[:int(0.9 * len(y_train))], y_train[int(0.9 * len(y_train)):]\n",
    "                    \n",
    "                    model = model_func(input_shape=(X_train_part.shape[1], X_train_part.shape[2]))\n",
    "                    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "                    \n",
    "                    # Train the Keras model with early stopping\n",
    "                    model.fit(X_train_part, y_train_part, epochs=200, batch_size=256, verbose=0, \n",
    "                              validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "                    \n",
    "                    y_pred_prob = model.predict(X_test)  # Predicted probabilities\n",
    "                \n",
    "                y_pred = (y_pred_prob > threshold).astype('int32')\n",
    "                \n",
    "                # Calculate metrics for this fold\n",
    "                precision = precision_score(y_test, y_pred)\n",
    "                recall = recall_score(y_test, y_pred)\n",
    "                f1 = f1_score(y_test, y_pred)  # Calculate F1 score\n",
    "                roc_auc = roc_auc_score(y_test, y_pred_prob)  # Calculate ROC-AUC score\n",
    "                \n",
    "                # Append scores\n",
    "                precision_scores.append(precision)\n",
    "                recall_scores.append(recall)\n",
    "                f1_scores.append(f1)  # Append F1 score\n",
    "                roc_auc_scores.append(roc_auc)  # Append ROC-AUC score\n",
    "            \n",
    "            # Compute and return mean scores across all folds\n",
    "            mean_precision = np.mean(precision_scores)\n",
    "            mean_recall = np.mean(recall_scores)\n",
    "            mean_f1 = np.mean(f1_scores)  # Mean F1 score\n",
    "            mean_roc_auc = np.mean(roc_auc_scores)  # Mean ROC-AUC score\n",
    "            \n",
    "            return mean_precision, mean_recall, mean_f1, mean_roc_auc\n",
    "        \n",
    "        # 1. Logistic Regression\n",
    "        def logistic_regression():\n",
    "            return LogisticRegression(max_iter=1000, random_state=42)\n",
    "        \n",
    "        # 2. Random Forest\n",
    "        def random_forest():\n",
    "            return RandomForestClassifier(random_state=42)\n",
    "        \n",
    "        # 3. SVM model\n",
    "        def svm_model():\n",
    "            return SVC(probability=True, random_state=42)\n",
    "            \n",
    "        # 4. PredictPTB model\n",
    "        def build_predict_ptb(input_shape):\n",
    "            inputs = Input(shape=input_shape)\n",
    "            visit_embedding = TimeDistributed(Dense(64, activation='relu'))(inputs)\n",
    "            rnn_beta, _ = GRU(32, return_sequences=True, return_state=True)(visit_embedding)\n",
    "            beta = TimeDistributed(Dense(64, activation='tanh'))(rnn_beta)\n",
    "            elementwise_prod = Multiply()([beta, visit_embedding])\n",
    "            context_vector = Lambda(lambda x: tf.reduce_sum(x, axis=1))(elementwise_prod)\n",
    "            output = Dense(1, activation='sigmoid')(context_vector)\n",
    "            model = Model(inputs=inputs, outputs=output)\n",
    "            model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy')\n",
    "            return model\n",
    "        \n",
    "        # 5. RETAIN model\n",
    "        def build_retain(input_shape):\n",
    "            inputs = Input(shape=input_shape)\n",
    "            visit_embedding = TimeDistributed(Dense(64, activation='relu'))(inputs)\n",
    "            rnn_alpha, _ = GRU(32, return_sequences=True, return_state=True)(visit_embedding)\n",
    "            attention_scores = TimeDistributed(Dense(1))(rnn_alpha)\n",
    "            alpha = Softmax(axis=1)(attention_scores)\n",
    "            rnn_beta, _ = GRU(32, return_sequences=True, return_state=True)(visit_embedding)\n",
    "            beta = TimeDistributed(Dense(64, activation='tanh'))(rnn_beta)\n",
    "            weighted_context = Multiply()([alpha, Multiply()([beta, visit_embedding])])\n",
    "            context_vector = Lambda(lambda x: tf.reduce_sum(x, axis=1))(weighted_context)\n",
    "            output = Dense(1, activation='sigmoid')(context_vector)\n",
    "            model = Model(inputs=inputs, outputs=output)\n",
    "            model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy')\n",
    "            return model\n",
    "        \n",
    "        # 6. GRU model\n",
    "        def build_gru(input_shape):\n",
    "            model = Sequential([\n",
    "                Input(shape=input_shape),\n",
    "                GRU(32, activation='relu'),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "            model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy')\n",
    "            return model\n",
    "        \n",
    "        #### 7. GRUD With Static Attention\n",
    "        class GRUDWithStaticAttention(nn.Module):\n",
    "            def __init__(self, input_size, hidden_size, output_size, mean_values, static_input_size, static_embedding_size, dropout_rate=0.5):\n",
    "                \"\"\"\n",
    "                GRU-D model with attention mechanism generated from static data.\n",
    "                \n",
    "                Args:\n",
    "                    input_size (int): Number of input features for temporal data.\n",
    "                    hidden_size (int): Number of hidden units in GRU.\n",
    "                    output_size (int): Number of output classes (for classification tasks).\n",
    "                    mean_values (torch.Tensor): Empirical mean values for each input feature.\n",
    "                    static_input_size (int): Number of input features for static data.\n",
    "                    static_embedding_size (int): Size of the embedding for static data.\n",
    "                \"\"\"\n",
    "                super(GRUDWithStaticAttention, self).__init__()\n",
    "                \n",
    "                # Temporal (GRU-D) parameters\n",
    "                self.input_size = input_size\n",
    "                self.hidden_size = hidden_size\n",
    "                self.output_size = output_size\n",
    "                \n",
    "                self.mean_values = mean_values\n",
    "                \n",
    "                self.gamma_x = nn.Linear(input_size, input_size)\n",
    "                \n",
    "                # GRU gate parameters (temporal part) with combined input, hidden state, and mask\n",
    "                self.zl = nn.Linear(input_size + hidden_size + input_size, hidden_size)  # Update gate\n",
    "                self.rl = nn.Linear(input_size + hidden_size + input_size, hidden_size)  # Reset gate\n",
    "                self.hl = nn.Linear(input_size + hidden_size + input_size, hidden_size)  # Candidate hidden state\n",
    "        \n",
    "                # Embedding or processing static data\n",
    "                self.static_fc = nn.Linear(static_input_size, static_embedding_size)\n",
    "        \n",
    "                # Attention mechanism based on static data\n",
    "                self.attention = nn.Linear(static_embedding_size, hidden_size)\n",
    "        \n",
    "                # Fully connected output layer\n",
    "                self.dropout = nn.Dropout(dropout_rate)\n",
    "                self.concat_fc = nn.Linear(hidden_size + static_embedding_size, output_size)\n",
    "\n",
    "        \n",
    "            def forward(self, x, m, delta, static_data):\n",
    "                \"\"\"\n",
    "                Forward pass of GRU-D with static data attention.\n",
    "                \n",
    "                Args:\n",
    "                    x (torch.Tensor): Temporal input data [batch_size, seq_len, input_size].\n",
    "                    m (torch.Tensor): Masking vector [batch_size, seq_len, input_size].\n",
    "                    delta (torch.Tensor): Time intervals [batch_size, seq_len, input_size].\n",
    "                    static_data (torch.Tensor): Static data input [batch_size, static_input_size].\n",
    "                \"\"\"\n",
    "                batch_size, seq_len, _ = x.size()\n",
    "                \n",
    "                # Process static data to generate attention weights\n",
    "                static_embed = torch.relu(self.static_fc(static_data))  # [batch_size, static_embedding_size]\n",
    "                \n",
    "                # Generate attention weights from static data\n",
    "                attention_weights = torch.sigmoid(self.attention(static_embed))  # [batch_size, hidden_size]\n",
    "                \n",
    "                # Initialize hidden state\n",
    "                h = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "                \n",
    "                for t in range(seq_len):\n",
    "                    # Corrected input decay mechanism\n",
    "                    gamma_x_t = torch.exp(-F.relu(self.gamma_x(delta[:, t, :])))\n",
    "                    x_t_hat = m[:, t, :] * x[:, t, :] + (1 - m[:, t, :]) * ((1 - gamma_x_t) * self.mean_values + gamma_x_t * x[:, t, :])\n",
    "                    \n",
    "                    # Concatenate input, hidden state, and mask\n",
    "                    combined = torch.cat([x_t_hat, h, m[:, t, :]], dim=-1)\n",
    "        \n",
    "                    # GRU gates\n",
    "                    r_t = torch.sigmoid(self.rl(combined))  # Reset gate\n",
    "                    z_t = torch.sigmoid(self.zl(combined))  # Update gate\n",
    "                    h_tilde = torch.tanh(self.hl(torch.cat([x_t_hat, r_t * h, m[:, t, :]], dim=-1)))  # Candidate hidden state\n",
    "        \n",
    "                    # Update hidden state\n",
    "                    h = (1 - z_t) * h + z_t * h_tilde\n",
    "        \n",
    "                # Apply attention weights to the final hidden state\n",
    "                h_weighted = attention_weights * h  # [batch_size, hidden_size]\n",
    "\n",
    "                h_concat = self.dropout(torch.cat([h_weighted, static_embed], dim=-1))  # [B, hidden + static_embedding_size]\n",
    "            \n",
    "                output = torch.sigmoid(self.concat_fc(h_concat)) \n",
    "                return output\n",
    "        \n",
    "        # Training Parameters\n",
    "        input_size = X_temporal_keras.shape[2]  # Temporal input features\n",
    "        hidden_size = 32  # GRU hidden state size\n",
    "        output_size = 1  # Binary classification\n",
    "        static_input_size = X_static_keras.shape[1]  # Static input features\n",
    "        static_embedding_size = 32  # Embedding size for static features\n",
    "        mean_values = torch.tensor(np.nanmean(X_temporal_keras, axis=(0, 1)), dtype=torch.float32)  # Temporal feature means\n",
    "        \n",
    "        \n",
    "        # Convert data to PyTorch tensors\n",
    "        x = torch.tensor(X_temporal_keras, dtype=torch.float32)  # Temporal input\n",
    "        mask = torch.tensor(X_temporal_mask_keras, dtype=torch.float32)  # Masking vector\n",
    "        delta = torch.tensor(X_temporal_delta_keras, dtype=torch.float32)  # Time intervals\n",
    "        static_data = torch.tensor(X_static_keras, dtype=torch.float32)  # Static input\n",
    "        y = torch.tensor(y_keras, dtype=torch.float32)  # Binary labels\n",
    "        \n",
    "        # 8. XGBoost model\n",
    "        def xgboost_model():\n",
    "            return XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "        \n",
    "        #### 9. GRUD\n",
    "        class GRUD(nn.Module):\n",
    "            def __init__(self, input_size, hidden_size, output_size, mean_values):\n",
    "                \"\"\"\n",
    "                GRU-D model with attention mechanism generated from static data.\n",
    "                \n",
    "                Args:\n",
    "                    input_size (int): Number of input features for temporal data.\n",
    "                    hidden_size (int): Number of hidden units in GRU.\n",
    "                    output_size (int): Number of output classes (for classification tasks).\n",
    "                    mean_values (torch.Tensor): Empirical mean values for each input feature.\n",
    "                \"\"\"\n",
    "                super(GRUD, self).__init__()\n",
    "                \n",
    "                # Temporal (GRU-D) parameters\n",
    "                self.input_size = input_size\n",
    "                self.hidden_size = hidden_size\n",
    "                self.output_size = output_size\n",
    "                \n",
    "                self.mean_values = mean_values\n",
    "                \n",
    "                self.gamma_x = nn.Linear(input_size, input_size)\n",
    "                \n",
    "                # GRU gate parameters (temporal part) with combined input, hidden state, and mask\n",
    "                self.zl = nn.Linear(input_size + hidden_size + input_size, hidden_size)  # Update gate\n",
    "                self.rl = nn.Linear(input_size + hidden_size + input_size, hidden_size)  # Reset gate\n",
    "                self.hl = nn.Linear(input_size + hidden_size + input_size, hidden_size)  # Candidate hidden state\n",
    "        \n",
    "                # Fully connected output layer\n",
    "                self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "            def forward(self, x, m, delta):\n",
    "                \"\"\"\n",
    "                Forward pass of GRU-D with static data attention.\n",
    "                \n",
    "                Args:\n",
    "                    x (torch.Tensor): Temporal input data [batch_size, seq_len, input_size].\n",
    "                    m (torch.Tensor): Masking vector [batch_size, seq_len, input_size].\n",
    "                    delta (torch.Tensor): Time intervals [batch_size, seq_len, input_size].\n",
    "                \"\"\"\n",
    "                batch_size, seq_len, _ = x.size()\n",
    "                      \n",
    "                # Initialize hidden state\n",
    "                h = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "                \n",
    "                for t in range(seq_len):\n",
    "                    # Corrected input decay mechanism\n",
    "                    gamma_x_t = torch.exp(-F.relu(self.gamma_x(delta[:, t, :])))\n",
    "                    x_t_hat = m[:, t, :] * x[:, t, :] + (1 - m[:, t, :]) * ((1 - gamma_x_t) * self.mean_values + gamma_x_t * x[:, t, :])\n",
    "                    \n",
    "                    # Concatenate input, hidden state, and mask\n",
    "                    combined = torch.cat([x_t_hat, h, m[:, t, :]], dim=-1)\n",
    "        \n",
    "                    # GRU gates\n",
    "                    r_t = torch.sigmoid(self.rl(combined))  # Reset gate\n",
    "                    z_t = torch.sigmoid(self.zl(combined))  # Update gate\n",
    "                    h_tilde = torch.tanh(self.hl(torch.cat([x_t_hat, r_t * h, m[:, t, :]], dim=-1)))  # Candidate hidden state\n",
    "        \n",
    "                    # Update hidden state\n",
    "                    h = (1 - z_t) * h + z_t * h_tilde\n",
    "        \n",
    "                # Final output\n",
    "                output = self.fc(h)\n",
    "        \n",
    "                # Sigmoid for binary classification\n",
    "                output = torch.sigmoid(output)\n",
    "                \n",
    "                return output\n",
    "        \n",
    "        def GRUD_Static_AUC(model_class, x, mask, delta, static_data, y, n_folds=5, n_epochs=200, learning_rate=0.01):\n",
    "            \"\"\"\n",
    "            Perform 5-fold cross-validation for the GRUDWithStaticAttention model with ROC-AUC.\n",
    "            \n",
    "            Args:\n",
    "                model_class: The class of the model to instantiate.\n",
    "                x (torch.Tensor): Temporal input data [batch_size, seq_len, input_size].\n",
    "                mask (torch.Tensor): Masking vector [batch_size, seq_len, input_size].\n",
    "                delta (torch.Tensor): Time intervals [batch_size, seq_len, input_size].\n",
    "                static_data (torch.Tensor): Static input data [batch_size, static_input_size].\n",
    "                y (torch.Tensor): Binary labels for classification [batch_size, 1].\n",
    "                n_folds (int): Number of folds for cross-validation.\n",
    "                n_epochs (int): Number of epochs for training.\n",
    "                learning_rate (float): Learning rate for optimizer.\n",
    "            \n",
    "            Returns:\n",
    "                mean_precision, mean_recall, mean_f1, mean_roc_auc: Mean metrics across folds.\n",
    "            \"\"\"\n",
    "            kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "            fold_results = []\n",
    "        \n",
    "            for fold, (train_index, test_index) in enumerate(kf.split(x)):\n",
    "                print(f\"Fold {fold + 1}/{n_folds}\")\n",
    "        \n",
    "                # Split data into train and test sets\n",
    "                x_train, x_test = x[train_index], x[test_index]\n",
    "                mask_train, mask_test = mask[train_index], mask[test_index]\n",
    "                delta_train, delta_test = delta[train_index], delta[test_index]\n",
    "                static_train, static_test = static_data[train_index], static_data[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "                # Initialize the model\n",
    "                model = model_class(\n",
    "                    input_size=x.shape[2],\n",
    "                    hidden_size=hidden_size,\n",
    "                    output_size=output_size,\n",
    "                    mean_values=mean_values,\n",
    "                    static_input_size=static_input_size,\n",
    "                    static_embedding_size=static_embedding_size,\n",
    "                )\n",
    "        \n",
    "                # Binary Cross-Entropy Loss and Adam Optimizer\n",
    "                criterion = nn.BCELoss()\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "                # Train the model\n",
    "                for epoch in range(n_epochs):\n",
    "                    model.train()\n",
    "                    optimizer.zero_grad()\n",
    "        \n",
    "                    # Forward pass\n",
    "                    outputs = model(x_train, mask_train, delta_train, static_train)\n",
    "        \n",
    "                    # Compute loss\n",
    "                    loss = criterion(outputs.squeeze(), y_train.squeeze())\n",
    "        \n",
    "                    # Backward pass and optimization\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "        \n",
    "                # Evaluate the model on test set\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(x_test, mask_test, delta_test, static_test).squeeze()\n",
    "                    probabilities = outputs.cpu().numpy()\n",
    "                    predictions = (probabilities > threshold).astype(int)\n",
    "                    y_test_numpy = y_test.int().cpu().numpy()\n",
    "        \n",
    "                    # Compute evaluation metrics\n",
    "                    precision = precision_score(y_test_numpy, predictions, zero_division=0)\n",
    "                    recall = recall_score(y_test_numpy, predictions, zero_division=0)\n",
    "                    f1 = f1_score(y_test_numpy, predictions, zero_division=0)\n",
    "                    roc_auc = roc_auc_score(y_test_numpy, probabilities)\n",
    "        \n",
    "                    fold_results.append({\n",
    "                        \"precision\": precision,\n",
    "                        \"recall\": recall,\n",
    "                        \"f1_score\": f1,\n",
    "                        \"roc_auc\": roc_auc\n",
    "                    })\n",
    "        \n",
    "                    print(f\"Fold {fold + 1} - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, ROC-AUC: {roc_auc:.4f}\")\n",
    "        \n",
    "            # Compute mean metrics across all folds\n",
    "            mean_precision = np.mean([result['precision'] for result in fold_results])\n",
    "            mean_recall = np.mean([result['recall'] for result in fold_results])\n",
    "            mean_f1 = np.mean([result['f1_score'] for result in fold_results])\n",
    "            mean_roc_auc = np.mean([result['roc_auc'] for result in fold_results])\n",
    "        \n",
    "            return mean_precision, mean_recall, mean_f1, mean_roc_auc\n",
    "        def GRUD_AUC(model_class, x, mask, delta, y, n_folds=5, n_epochs=200, learning_rate=0.01):\n",
    "            \"\"\"\n",
    "            Perform 5-fold cross-validation for the GRUD model with ROC-AUC.\n",
    "            \n",
    "            Args:\n",
    "                model_class: The class of the model to instantiate.\n",
    "                x (torch.Tensor): Temporal input data [batch_size, seq_len, input_size].\n",
    "                mask (torch.Tensor): Masking vector [batch_size, seq_len, input_size].\n",
    "                delta (torch.Tensor): Time intervals [batch_size, seq_len, input_size].\n",
    "                y (torch.Tensor): Binary labels for classification [batch_size, 1].\n",
    "                n_folds (int): Number of folds for cross-validation.\n",
    "                n_epochs (int): Number of epochs for training.\n",
    "                learning_rate (float): Learning rate for optimizer.\n",
    "            \n",
    "            Returns:\n",
    "                mean_precision, mean_recall, mean_f1, mean_roc_auc: Mean metrics across folds.\n",
    "            \"\"\"\n",
    "            kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "            fold_results = []\n",
    "        \n",
    "            for fold, (train_index, test_index) in enumerate(kf.split(x)):\n",
    "                print(f\"Fold {fold + 1}/{n_folds}\")\n",
    "        \n",
    "                # Split data into train and test sets\n",
    "                x_train, x_test = x[train_index], x[test_index]\n",
    "                mask_train, mask_test = mask[train_index], mask[test_index]\n",
    "                delta_train, delta_test = delta[train_index], delta[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "                # Initialize the model\n",
    "                model = model_class(\n",
    "                    input_size=x.shape[2],\n",
    "                    hidden_size=hidden_size,\n",
    "                    output_size=output_size,\n",
    "                    mean_values=mean_values,\n",
    "                )\n",
    "        \n",
    "                # Binary Cross-Entropy Loss and Adam Optimizer\n",
    "                criterion = nn.BCELoss()\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "                # Train the model\n",
    "                for epoch in range(n_epochs):\n",
    "                    model.train()\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = model(x_train, mask_train, delta_train)\n",
    "        \n",
    "                    # Compute loss\n",
    "                    loss = criterion(outputs.squeeze(), y_train.squeeze())\n",
    "        \n",
    "                    # Backward pass and optimization\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "        \n",
    "                # Evaluate the model on test set\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(x_test, mask_test, delta_test).squeeze()\n",
    "                    probabilities = outputs.cpu().numpy()\n",
    "                    predictions = (probabilities > threshold).astype(int)\n",
    "                    y_test_numpy = y_test.int().cpu().numpy()\n",
    "        \n",
    "                    # Compute evaluation metrics\n",
    "                    precision = precision_score(y_test_numpy, predictions, zero_division=0)\n",
    "                    recall = recall_score(y_test_numpy, predictions, zero_division=0)\n",
    "                    f1 = f1_score(y_test_numpy, predictions, zero_division=0)\n",
    "                    roc_auc = roc_auc_score(y_test_numpy, probabilities)\n",
    "        \n",
    "                    fold_results.append({\n",
    "                        \"precision\": precision,\n",
    "                        \"recall\": recall,\n",
    "                        \"f1_score\": f1,\n",
    "                        \"roc_auc\": roc_auc\n",
    "                    })\n",
    "        \n",
    "                    print(f\"Fold {fold + 1} - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, ROC-AUC: {roc_auc:.4f}\")\n",
    "        \n",
    "            # Compute mean metrics across all folds\n",
    "            mean_precision = np.mean([result['precision'] for result in fold_results])\n",
    "            mean_recall = np.mean([result['recall'] for result in fold_results])\n",
    "            mean_f1 = np.mean([result['f1_score'] for result in fold_results])\n",
    "            mean_roc_auc = np.mean([result['roc_auc'] for result in fold_results])\n",
    "        \n",
    "            return mean_precision, mean_recall, mean_f1, mean_roc_auc\n",
    "        \n",
    "        \n",
    "        # Prepare lists to store the results\n",
    "        models = [\"Logistic Regression\", \"Random Forest\", \"SVM\", \"XGBoost\", \"PredictPTB\", \"RETAIN\", \"GRU-D-Static\"]\n",
    "        \n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f1_scores = []  # List for F1 scores\n",
    "        roc_aucs = []  # List for ROC-AUC scores\n",
    "        \n",
    "        # Function to append results from the evaluate_model function\n",
    "        def append_results(model_name, precision, recall, f1, roc_auc):\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "            roc_aucs.append(roc_auc)\n",
    "        \n",
    "        \n",
    "        # Evaluate Logistic Regression, Random Forest, SVM (2D input)\n",
    "        precision, recall, f1, roc_auc = evaluate_model(logistic_regression, X_sklearn, y_sklearn, model_type=\"sklearn\")\n",
    "        append_results(\"Logistic Regression\", precision, recall, f1, roc_auc)\n",
    "        \n",
    "        precision, recall, f1, roc_auc = evaluate_model(random_forest, X_sklearn, y_sklearn, model_type=\"sklearn\")\n",
    "        append_results(\"Random Forest\", precision, recall, f1, roc_auc)\n",
    "        \n",
    "        precision, recall, f1, roc_auc = evaluate_model(svm_model, X_sklearn, y_sklearn, model_type=\"sklearn\")\n",
    "        append_results(\"SVM\", precision, recall, f1, roc_auc)\n",
    "        \n",
    "        # Evaluate XGBoost (2D input for sklearn-based models)\n",
    "        precision, recall, f1, roc_auc = evaluate_model(xgboost_model, X_sklearn, y_sklearn, model_type=\"sklearn\")\n",
    "        append_results(\"XGBoost\", precision, recall, f1, roc_auc)\n",
    "        \n",
    "        # Evaluate PredictPTB, RETAIN, and GRU (3D input)\n",
    "        precision, recall, f1, roc_auc = evaluate_model(build_predict_ptb, X_keras, y_keras, model_type=\"keras\")\n",
    "        append_results(\"PredictPTB\", precision, recall, f1, roc_auc)\n",
    "        \n",
    "        precision, recall, f1, roc_auc = evaluate_model(build_retain, X_keras, y_keras, model_type=\"keras\")\n",
    "        append_results(\"RETAIN\", precision, recall, f1, roc_auc)\n",
    "        \n",
    "        # Evaluate GRU-D-Static (3D input)\n",
    "        precision, recall, f1, roc_auc = GRUD_Static_AUC(\n",
    "            GRUDWithStaticAttention,\n",
    "            x,\n",
    "            mask,\n",
    "            delta,\n",
    "            static_data,\n",
    "            y,\n",
    "            n_folds=5,\n",
    "            n_epochs=200,\n",
    "            learning_rate=0.01\n",
    "        )\n",
    "        append_results(\"GRU-D-Static\", precision, recall, f1, roc_auc)\n",
    "        \n",
    "        import matplotlib.pyplot as plt\n",
    "        # Step 3: Create individual vertical plots for each metric with flipped axes\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Plot for Precision\n",
    "        plt.subplot(1, 4, 1)\n",
    "        plt.bar(models, precisions, color='red')\n",
    "        plt.title('Precision by Model')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.ylabel('Precision')\n",
    "        plt.xticks(rotation=45, ha='right')  # Rotate and align x-ticks\n",
    "        \n",
    "        # Plot for Recall\n",
    "        plt.subplot(1, 4, 2)\n",
    "        plt.bar(models, recalls, color='orange')\n",
    "        plt.title('Recall by Model')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.ylabel('Recall')\n",
    "        plt.xticks(rotation=45, ha='right')  # Rotate and align x-ticks\n",
    "        \n",
    "        # Plot for F1 Score\n",
    "        plt.subplot(1, 4, 3)\n",
    "        plt.bar(models, f1_scores, color='blue')\n",
    "        plt.title('F1 Score by Model')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.xticks(rotation=45, ha='right')  # Rotate and align x-ticks\n",
    "        \n",
    "        # Plot for ROC-AUC\n",
    "        plt.subplot(1, 4, 4)\n",
    "        plt.bar(models, roc_aucs, color='purple')\n",
    "        plt.title('ROC-AUC by Model')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.ylabel('ROC-AUC')\n",
    "        plt.xticks(rotation=45, ha='right')  # Rotate and align x-ticks\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Create a summary DataFrame\n",
    "        results_summary = pd.DataFrame({\n",
    "            \"Model\": models,\n",
    "            \"Precision\": precisions,\n",
    "            \"Recall\": recalls,\n",
    "            \"F1 Score\": f1_scores,\n",
    "            \"ROC-AUC\": roc_aucs\n",
    "        })\n",
    "        \n",
    "        # Round the numerical values to 3 decimal places\n",
    "        results_summary = results_summary.round(3)\n",
    "\n",
    "        return(results_summary,threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcd12d4-f278-42d9-bf91-48f0264fad3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to store results\n",
    "results_best = []\n",
    "\n",
    "# Function to process results for an outcome\n",
    "def process_results(outcome):\n",
    "    # Run the function and capture the returned values\n",
    "    results_summary, threshold = print_results(outcome)\n",
    "\n",
    "    print(results_summary)\n",
    "    \n",
    "    # Extract best-performing models for each metric\n",
    "    best_precision = results_summary.loc[results_summary[\"Precision\"].idxmax(), [\"Model\", \"Precision\"]]\n",
    "    best_recall = results_summary.loc[results_summary[\"Recall\"].idxmax(), [\"Model\", \"Recall\"]]\n",
    "    best_f1 = results_summary.loc[results_summary[\"F1 Score\"].idxmax(), [\"Model\", \"F1 Score\"]]\n",
    "    best_roc_auc = results_summary.loc[results_summary[\"ROC-AUC\"].idxmax(), [\"Model\", \"ROC-AUC\"]]\n",
    "\n",
    "    # Append results to the list\n",
    "    results_best.append({\n",
    "        \"Outcome\": outcome,\n",
    "        \"Prevalence (%)\": f\"{threshold * 100:.2f}\",\n",
    "        \"Best Precision Model\": best_precision[\"Model\"],\n",
    "        \"Best Precision Score\": f\"{best_precision['Precision']:.3f}\",\n",
    "        \"Best Recall Model\": best_recall[\"Model\"],\n",
    "        \"Best Recall Score\": f\"{best_recall['Recall']:.3f}\",\n",
    "        \"Best F1 Score Model\": best_f1[\"Model\"],\n",
    "        \"Best F1 Score\": f\"{best_f1['F1 Score']:.3f}\",\n",
    "        \"Best ROC-AUC Model\": best_roc_auc[\"Model\"],\n",
    "        \"Best ROC-AUC Score\": f\"{best_roc_auc['ROC-AUC']:.3f}\"\n",
    "    })\n",
    "\n",
    "# List of outcomes to process\n",
    "outcomes = [\"STILLBIRTH_SIGNS_LIFE\",\"LBW2500_ANY\",\"INF_PSBI_IPC\",\"SVN\", \"NEARMISS\", \"NEO_DTH\"]\n",
    "\n",
    "# Run the function for each outcome\n",
    "for outcome in outcomes:\n",
    "    process_results(outcome)\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "summary_df = pd.DataFrame(results_best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
