{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de498c96-a7cb-4ae7-a0b1-ec516f92e161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, GRU, Input, Multiply, TimeDistributed, Softmax, Lambda, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from xgboost import XGBClassifier\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "import copy           \n",
    "\n",
    "def print_results(outcome_variable, lr_prob, proportion):\n",
    "        ### Training data:\n",
    "        # Load data from a CSV file\n",
    "        df = pd.read_csv('D:/Users/yipeng_wei/Documents/dl data/2025-10-31/df_dl_continuous_lr.csv')\n",
    "        df_mask = pd.read_csv('D:/Users/yipeng_wei/Documents/dl data/2025-10-31/df_dl_continuous_mask.csv')\n",
    "        df_delta = pd.read_csv('D:/Users/yipeng_wei/Documents/dl data/2025-10-31/df_dl_continuous_delta.csv')\n",
    "\n",
    "        Africa=[\"Zambia\", \"Kenya\", \"Ghana\"]\n",
    "        Asia=[\"Pakistan\", \"India-CMC\", \"India-SAS\"]\n",
    "    \n",
    "       # Drop rows with NaN in the target column before any operations\n",
    "        df = df.dropna(subset=[outcome_variable])\n",
    "        df = df[df['SITE'].isin(Asia)]\n",
    "        df = df.reset_index(drop=True)\n",
    "    \n",
    "        df_mask = df_mask.dropna(subset=[outcome_variable])\n",
    "        df_mask = df_mask[df_mask['SITE'].isin(Asia)]\n",
    "        df_mask = df_mask.reset_index(drop=True)\n",
    "    \n",
    "        df_delta = df_delta.dropna(subset=[outcome_variable])\n",
    "        df_delta = df_delta[df_delta['SITE'].isin(Asia)]\n",
    "        df_delta = df_delta.reset_index(drop=True)\n",
    "        \n",
    "        features_categorical = [\"TYPE_VISIT\",\n",
    "                    \"STILLBIRTH_IND\",\"PRETERM_IND\",\"CESARIAN_IND\",\"NUM_MISCARRIAGE_ind\",\n",
    "                    \"DEPR_EVER\",\n",
    "                    \"PAID_WORK\",\"GPARITY_2\",\"GPARITY_1\",\"WEALTH_QUINT_1\",\"WEALTH_QUINT_2\",\"WEALTH_QUINT_3\",\"WEALTH_QUINT_4\",\"SCHOOL_MORE10\",\"WATER_IMPROVED\",\"TOILET_IMPROVED\",\"STOVE_FUEL\",\"HH_SMOKE\",\"SMOKE\",\"CHEW_TOBACCO\",\"CHEW_BETELNUT\",\n",
    "                    \"BMI_LEVEL_ENROLL_underweight\",\"BMI_LEVEL_ENROLL_overweight\",\"BMI_LEVEL_ENROLL_obese\",\n",
    "                    \"GWG_ADEQUACY_inadequate\",\"GWG_ADEQUACY_excessive\",\n",
    "                    \"MAT_CES_ANY\",\"BIRTH_FACILITY\",\n",
    "                    \"HTN\",\"DIAB_OVERT_ANY\",\"HIV_POSITIVE\",\"TB_SYMP_POSITIVE\",\"MAL_POSITIVE\",\"HBV_POSITIVE_ENROLL\",\"HCV_POSITIVE_ENROLL\",\"STI_POSITIVE\",\n",
    "                    \"GES_HTN\",\"DIAB_GEST_ANY\",\n",
    "                    \"INF_SEX\",\"INF_HYPERBILI_NICE\",\"BREASTFED\",\n",
    "                    \"NUM_FETUS_2\",\"NUM_FETUS_3\",\"INF_ANOMALY\",\"PREVIA\",\n",
    "                    \"INF_PSBI_ANY\",\n",
    "                    \"M08_ANEMIA\"]\n",
    "        \n",
    "        features_continuous = [\"MAT_AGE\",\"MUAC_ENROLL\",\"BMI_ENROLL\",\"GESTAGEBIRTH_ANY\",\"BWEIGHT_ANY\",\"CROWDING_IND\",\n",
    "                              \"M01_AFI_PERES\",\n",
    "                              \"M08_IRON_TOT_UGDL_LBORRES\", \"M08_FERRITIN_LBORRES\",\"M08_IRON_HEP_LBORRES\",\"M08_CBC_HB_LBORRES\"]\n",
    "        \n",
    "        features_static = [\"STILLBIRTH_IND\",\"PRETERM_IND\",\"CESARIAN_IND\",\"NUM_MISCARRIAGE_ind\",\n",
    "                    \"DEPR_EVER\",\n",
    "                    \"PAID_WORK\",\"GPARITY_2\",\"GPARITY_1\",\"WEALTH_QUINT_1\",\"WEALTH_QUINT_2\",\"WEALTH_QUINT_3\",\"WEALTH_QUINT_4\",\"SCHOOL_MORE10\",\"WATER_IMPROVED\",\"TOILET_IMPROVED\",\"STOVE_FUEL\",\"HH_SMOKE\",\"SMOKE\",\"CHEW_TOBACCO\",\"CHEW_BETELNUT\",\"CROWDING_IND\",\n",
    "                    \"BMI_LEVEL_ENROLL_underweight\",\"BMI_LEVEL_ENROLL_overweight\",\"BMI_LEVEL_ENROLL_obese\",\n",
    "                    \"GWG_ADEQUACY_inadequate\",\"GWG_ADEQUACY_excessive\",\n",
    "                    \"MAT_CES_ANY\",\"BIRTH_FACILITY\",\n",
    "                    \"HTN\",\"DIAB_OVERT_ANY\",\"HIV_POSITIVE\",\"TB_SYMP_POSITIVE\",\"MAL_POSITIVE\",\"HBV_POSITIVE_ENROLL\",\"HCV_POSITIVE_ENROLL\",\"STI_POSITIVE\",\n",
    "                    \"GES_HTN\",\"DIAB_GEST_ANY\",\n",
    "                    \"INF_SEX\",\"INF_HYPERBILI_NICE\",\"BREASTFED\",\"GESTAGEBIRTH_ANY\",\"BWEIGHT_ANY\",\n",
    "                    \"INF_ANOMALY\",\n",
    "                    \"INF_PSBI_ANY\",\n",
    "                    \"NUM_FETUS_2\",\"NUM_FETUS_3\",\"MAT_AGE\",\"MUAC_ENROLL\",\n",
    "                    \"BMI_ENROLL\"]\n",
    "        \n",
    "        features_temporal = [\"TYPE_VISIT\",\n",
    "                             \"M01_AFI_PERES\",\n",
    "                             \"PREVIA\",\n",
    "                             \"M08_IRON_TOT_UGDL_LBORRES\",\"M08_FERRITIN_LBORRES\",\"M08_IRON_HEP_LBORRES\",\"M08_CBC_HB_LBORRES\",\"M08_ANEMIA\"]\n",
    "\n",
    "        features_to_remove_by_outcome = {\n",
    "        \"STILLBIRTH_SIGNS_LIFE\": [\"INF_HYPERBILI_NICE\",\"BREASTFED\",\"BWEIGHT_ANY\",\"INF_PSBI_ANY\"],\n",
    "        \"PRETERMBIRTH_LT37\": [\"INF_HYPERBILI_NICE\",\"BREASTFED\",\"GESTAGEBIRTH_ANY\",\"BWEIGHT_ANY\",\"INF_PSBI_ANY\",\"MAT_CES_ANY\",\"BIRTH_FACILITY\"],\n",
    "        \"LBW2500_ANY\": [\"INF_HYPERBILI_NICE\",\"BREASTFED\",\"BWEIGHT_ANY\",\"INF_PSBI_ANY\",\"MAT_CES_ANY\",\"BIRTH_FACILITY\"],\n",
    "        \"INF_PSBI_OUTCOME\": [\"INF_HYPERBILI_NICE\",\"INF_PSBI_ANY\",\"PREVIA\"],\n",
    "        \"SVN\": [\"INF_SEX\",\"INF_HYPERBILI_NICE\",\"GESTAGEBIRTH_ANY\",\"BWEIGHT_ANY\",\"INF_PSBI_ANY\",\"BREASTFED\",\"MAT_CES_ANY\",\"BIRTH_FACILITY\"],\n",
    "        \"NEARMISS\": [\"INF_SEX\",\"INF_HYPERBILI_NICE\",\"BREASTFED\",\"GESTAGEBIRTH_ANY\",\"BWEIGHT_ANY\",\"INF_PSBI_ANY\"]\n",
    "        }\n",
    "    \n",
    "        features_to_remove = features_to_remove_by_outcome.get(outcome_variable, [])\n",
    "        \n",
    "        # Remove features from the lists\n",
    "        features_categorical = [f for f in features_categorical if f not in features_to_remove]\n",
    "        features_continuous = [f for f in features_continuous if f not in features_to_remove]\n",
    "        features_static = [f for f in features_static if f not in features_to_remove]\n",
    "        features_temporal = [f for f in features_temporal if f not in features_to_remove]\n",
    "        \n",
    "        # Separate categorical and continuous features\n",
    "        df_categorical = df[features_categorical]\n",
    "        df_continuous = df[features_continuous]\n",
    "        \n",
    "        # Rescale continuous features to (0, 1) range\n",
    "        lower_q = 0.02   # 2nd percentile\n",
    "        upper_q = 0.98   # 98th percentile\n",
    "        \n",
    "        df_continuous_capped = df_continuous.copy()\n",
    "        \n",
    "        for col in features_continuous:\n",
    "            lower = df_continuous[col].quantile(lower_q)\n",
    "            upper = df_continuous[col].quantile(upper_q)\n",
    "            df_continuous_capped[col] = df_continuous[col].clip(lower, upper)\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        df_continuous_scaled = pd.DataFrame(\n",
    "            scaler.fit_transform(df_continuous_capped),\n",
    "            columns=features_continuous,\n",
    "            index=df.index\n",
    "        )\n",
    "        \n",
    "        # Concatenate scaled continuous features with categorical features\n",
    "        X = pd.concat([df_categorical, df_continuous_scaled], axis=1)\n",
    "        \n",
    "        ##Static and temporal data\n",
    "        X_static = X[features_static]\n",
    "        \n",
    "        X_temporal = X[features_temporal]\n",
    "        X_temporal_delta = df_delta[features_temporal]\n",
    "        X_temporal_mask = df_mask[features_temporal]\n",
    "        \n",
    "        y = df[outcome_variable]\n",
    "        \n",
    "        # One-hot encoding for non-binary categorical features in X\n",
    "        X = pd.get_dummies(X, columns=['TYPE_VISIT'], dummy_na=False)\n",
    "        X_temporal = pd.get_dummies(X_temporal, columns=['TYPE_VISIT'], dummy_na=False)\n",
    "        X_temporal_delta = pd.get_dummies(X_temporal_delta, columns=['TYPE_VISIT'], dummy_na=False)\n",
    "        X_temporal_mask = pd.get_dummies(X_temporal_mask, columns=['TYPE_VISIT'], dummy_na=False)\n",
    "        \n",
    "        # Preprocess for Keras models (3D matrix)\n",
    "        n_samples = X_temporal.shape[0] // 5 \n",
    "        X_static_keras = X_static.iloc[::5].values.astype('float32')\n",
    "        \n",
    "        X_temporal_keras = X_temporal.values.reshape(n_samples, 5, X_temporal.shape[1]).astype('float32')\n",
    "        X_temporal_delta_keras = X_temporal_delta.values.reshape(n_samples, 5, X_temporal_delta.shape[1]).astype('float32')\n",
    "        X_temporal_mask_keras = X_temporal_mask.values.reshape(n_samples, 5, X_temporal_mask.shape[1]).astype('float32')\n",
    "        \n",
    "        X_keras = X.values.reshape(n_samples, 5, X.shape[1]).astype('float32')\n",
    "        \n",
    "        # Align y for Keras models\n",
    "        y_keras = y.iloc[::5].values.astype('float32')\n",
    "\n",
    "        ### Target data:\n",
    "        # Load data from a CSV file\n",
    "        df_target = pd.read_csv('D:/Users/yipeng_wei/Documents/dl data/2025-10-31/df_dl_continuous_Africa_train.csv')\n",
    "        df_target_mask = pd.read_csv('D:/Users/yipeng_wei/Documents/dl data/2025-10-31/df_dl_continuous_mask_Africa_train.csv')\n",
    "        df_target_delta = pd.read_csv('D:/Users/yipeng_wei/Documents/dl data/2025-10-31/df_dl_continuous_delta_Africa_train.csv')\n",
    "        \n",
    "       # Drop rows with NaN in the target column before any operations\n",
    "        df_target = df_target.dropna(subset=[outcome_variable])\n",
    "        df_target = df_target.reset_index(drop=True)\n",
    "    \n",
    "        df_target_mask = df_target_mask.dropna(subset=[outcome_variable])\n",
    "        df_target_mask = df_target_mask.reset_index(drop=True)\n",
    "    \n",
    "        df_target_delta = df_target_delta.dropna(subset=[outcome_variable])\n",
    "        df_target_delta = df_target_delta.reset_index(drop=True)\n",
    "    \n",
    "        # Separate categorical and continuous features\n",
    "        df_target_categorical = df_target[features_categorical]\n",
    "        df_target_continuous = df_target[features_continuous]\n",
    "        \n",
    "        # get the effective train bounds (same as the ones used in training)\n",
    "        cap_bounds = {col: (df_continuous[col].quantile(lower_q), df_continuous[col].quantile(upper_q))\n",
    "                      for col in features_continuous}\n",
    "        \n",
    "        df_target_continuous_capped = df_target_continuous.copy()\n",
    "        for col in features_continuous:\n",
    "            lower, upper = cap_bounds[col]\n",
    "            df_target_continuous_capped[col] = df_target_continuous[col].clip(lower, upper)\n",
    "        \n",
    "        df_target_continuous_scaled = pd.DataFrame(\n",
    "            scaler.transform(df_target_continuous_capped),\n",
    "            columns=features_continuous,\n",
    "            index=df_target_continuous.index\n",
    "        )\n",
    "        \n",
    "        # Concatenate scaled continuous features with categorical features\n",
    "        X_target = pd.concat([df_target_categorical, df_target_continuous_scaled], axis=1)\n",
    "          \n",
    "        ##Static and temporal data\n",
    "        X_target_static = X_target[features_static]\n",
    "        \n",
    "        X_target_temporal = X_target[features_temporal]\n",
    "        X_target_temporal_delta = df_target_delta[features_temporal]\n",
    "        X_target_temporal_mask = df_target_mask[features_temporal]\n",
    "\n",
    "        y_target_soft = df_target[lr_prob]\n",
    "        y_target = df_target[outcome_variable]\n",
    "        threshold = (df_target[outcome_variable] == 1).mean()\n",
    "        \n",
    "        # One-hot encoding for non-binary categorical features in X\n",
    "        X_target = pd.get_dummies(X_target, columns=['TYPE_VISIT'], dummy_na=False)\n",
    "        X_target_temporal = pd.get_dummies(X_target_temporal, columns=['TYPE_VISIT'], dummy_na=False)\n",
    "        X_target_temporal_delta = pd.get_dummies(X_target_temporal_delta, columns=['TYPE_VISIT'], dummy_na=False)\n",
    "        X_target_temporal_mask = pd.get_dummies(X_target_temporal_mask, columns=['TYPE_VISIT'], dummy_na=False)\n",
    "        \n",
    "        # Preprocess for Keras models (3D matrix)\n",
    "        n_target_samples = X_target_temporal.shape[0] // 5 \n",
    "        X_target_static_keras = X_target_static.iloc[::5].values.astype('float32')\n",
    "        \n",
    "        X_target_temporal_keras = X_target_temporal.values.reshape(n_target_samples, 5, X_target_temporal.shape[1]).astype('float32')\n",
    "        X_target_temporal_delta_keras = X_target_temporal_delta.values.reshape(n_target_samples, 5, X_target_temporal_delta.shape[1]).astype('float32')\n",
    "        X_target_temporal_mask_keras = X_target_temporal_mask.values.reshape(n_target_samples, 5, X_target_temporal_mask.shape[1]).astype('float32')\n",
    "        \n",
    "        X_target_keras = X_target.values.reshape(n_target_samples, 5, X_target.shape[1]).astype('float32')\n",
    "        \n",
    "        # Align y for Keras models\n",
    "        y_target_soft_keras = y_target_soft.iloc[::5].values.astype('float32')\n",
    "\n",
    "        ### Test data:\n",
    "        # Load data from a CSV file\n",
    "        df_test = pd.read_csv('D:/Users/yipeng_wei/Documents/dl data/2025-10-31/df_dl_continuous_Africa.csv')\n",
    "        df_test_mask = pd.read_csv('D:/Users/yipeng_wei/Documents/dl data/2025-10-31/df_dl_continuous_mask_Africa.csv')\n",
    "        df_test_delta = pd.read_csv('D:/Users/yipeng_wei/Documents/dl data/2025-10-31/df_dl_continuous_delta_Africa.csv')\n",
    "        \n",
    "        # Drop rows with NaN in the test column before any operations\n",
    "        df_test = df_test.dropna(subset=[outcome_variable])\n",
    "        df_test = df_test.reset_index(drop=True)\n",
    "    \n",
    "        df_test_mask = df_test_mask.dropna(subset=[outcome_variable])\n",
    "        df_test_mask = df_test_mask.reset_index(drop=True)\n",
    "    \n",
    "        df_test_delta = df_test_delta.dropna(subset=[outcome_variable])\n",
    "        df_test_delta = df_test_delta.reset_index(drop=True)\n",
    "    \n",
    "        # Separate categorical and continuous features\n",
    "        df_test_categorical = df_test[features_categorical]\n",
    "        df_test_continuous = df_test[features_continuous]\n",
    "        \n",
    "        # get the effective train bounds (same as the ones used in training)\n",
    "        cap_bounds = {col: (df_continuous[col].quantile(lower_q), df_continuous[col].quantile(upper_q))\n",
    "                      for col in features_continuous}\n",
    "        \n",
    "        df_test_continuous_capped = df_test_continuous.copy()\n",
    "        for col in features_continuous:\n",
    "            lower, upper = cap_bounds[col]\n",
    "            df_test_continuous_capped[col] = df_test_continuous[col].clip(lower, upper)\n",
    "        \n",
    "        df_test_continuous_scaled = pd.DataFrame(\n",
    "            scaler.transform(df_test_continuous_capped),\n",
    "            columns=features_continuous,\n",
    "            index=df_test_continuous.index\n",
    "        )\n",
    "        \n",
    "        # Concatenate scaled continuous features with categorical features\n",
    "        X_test = pd.concat([df_test_categorical, df_test_continuous_scaled], axis=1)\n",
    "          \n",
    "        ##Static and temporal data\n",
    "        X_test_static = X_test[features_static]\n",
    "        \n",
    "        X_test_temporal = X_test[features_temporal]\n",
    "        X_test_temporal_delta = df_test_delta[features_temporal]\n",
    "        X_test_temporal_mask = df_test_mask[features_temporal]\n",
    "\n",
    "        y_test = df_test[outcome_variable]\n",
    "        threshold = (df_test[outcome_variable] == 1).mean()\n",
    "        \n",
    "        # One-hot encoding for non-binary categorical features in X\n",
    "        X_test = pd.get_dummies(X_test, columns=['TYPE_VISIT'], dummy_na=False)\n",
    "        X_test_temporal = pd.get_dummies(X_test_temporal, columns=['TYPE_VISIT'], dummy_na=False)\n",
    "        X_test_temporal_delta = pd.get_dummies(X_test_temporal_delta, columns=['TYPE_VISIT'], dummy_na=False)\n",
    "        X_test_temporal_mask = pd.get_dummies(X_test_temporal_mask, columns=['TYPE_VISIT'], dummy_na=False)\n",
    "        \n",
    "        # Preprocess for Keras models (3D matrix)\n",
    "        n_test_samples = X_test_temporal.shape[0] // 5 \n",
    "        X_test_static_keras = X_test_static.iloc[::5].values.astype('float32')\n",
    "        \n",
    "        X_test_temporal_keras = X_test_temporal.values.reshape(n_test_samples, 5, X_test_temporal.shape[1]).astype('float32')\n",
    "        X_test_temporal_delta_keras = X_test_temporal_delta.values.reshape(n_test_samples, 5, X_test_temporal_delta.shape[1]).astype('float32')\n",
    "        X_test_temporal_mask_keras = X_test_temporal_mask.values.reshape(n_test_samples, 5, X_test_temporal_mask.shape[1]).astype('float32')\n",
    "        \n",
    "        X_test_keras = X_test.values.reshape(n_test_samples, 5, X_test.shape[1]).astype('float32')\n",
    "        \n",
    "        # Align y for Keras models\n",
    "        y_test_keras = y_test.iloc[::5].values.astype('float32')\n",
    "\n",
    "        # Set random seed for reproducibility\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Total number of samples\n",
    "        n_select = X_target_static_keras.shape[0]\n",
    "        \n",
    "        # Randomly select 10% of indices\n",
    "        selected_indices = np.random.choice(n_select, size=int(n_select * proportion), replace=False)\n",
    "    \n",
    "        # Sort indices (optional, for consistent ordering)\n",
    "        selected_indices = np.sort(selected_indices)\n",
    "        \n",
    "        # Subset all arrays using the same indices\n",
    "        X_target_static_keras_sub = X_target_static_keras[selected_indices]\n",
    "        X_target_temporal_keras_sub = X_target_temporal_keras[selected_indices]\n",
    "        X_target_temporal_delta_keras_sub = X_target_temporal_delta_keras[selected_indices]\n",
    "        X_target_temporal_mask_keras_sub = X_target_temporal_mask_keras[selected_indices]\n",
    "        y_target_soft_keras_sub = y_target_soft_keras[selected_indices]\n",
    "\n",
    "        #### GRUD With Static Attention\n",
    "        class GRUDWithStaticAttention(nn.Module):\n",
    "            def __init__(self, input_size, hidden_size, output_size, mean_values, static_input_size, static_embedding_size, dropout_rate=0.5):\n",
    "                \"\"\"\n",
    "                GRU-D model with attention mechanism generated from static data.\n",
    "                \n",
    "                Args:\n",
    "                    input_size (int): Number of input features for temporal data.\n",
    "                    hidden_size (int): Number of hidden units in GRU.\n",
    "                    output_size (int): Number of output classes (for classification tasks).\n",
    "                    mean_values (torch.Tensor): Empirical mean values for each input feature.\n",
    "                    static_input_size (int): Number of input features for static data.\n",
    "                    static_embedding_size (int): Size of the embedding for static data.\n",
    "                \"\"\"\n",
    "                super(GRUDWithStaticAttention, self).__init__()\n",
    "                \n",
    "                # Temporal (GRU-D) parameters\n",
    "                self.input_size = input_size\n",
    "                self.hidden_size = hidden_size\n",
    "                self.output_size = output_size\n",
    "                \n",
    "                self.mean_values = mean_values\n",
    "                \n",
    "                self.gamma_x = nn.Linear(input_size, input_size)\n",
    "                \n",
    "                # GRU gate parameters (temporal part) with combined input, hidden state, and mask\n",
    "                self.zl = nn.Linear(input_size + hidden_size + input_size, hidden_size)  # Update gate\n",
    "                self.rl = nn.Linear(input_size + hidden_size + input_size, hidden_size)  # Reset gate\n",
    "                self.hl = nn.Linear(input_size + hidden_size + input_size, hidden_size)  # Candidate hidden state\n",
    "        \n",
    "                # Embedding or processing static data\n",
    "                self.static_fc = nn.Linear(static_input_size, static_embedding_size)\n",
    "        \n",
    "                # Attention mechanism based on static data\n",
    "                self.attention = nn.Linear(static_embedding_size, hidden_size)\n",
    "        \n",
    "                # Fully connected output layer\n",
    "                self.dropout = nn.Dropout(dropout_rate)\n",
    "                self.concat_fc = nn.Linear(hidden_size + static_embedding_size, output_size)\n",
    "\n",
    "        \n",
    "            def forward(self, x, m, delta, static_data):\n",
    "                \"\"\"\n",
    "                Forward pass of GRU-D with static data attention.\n",
    "                \n",
    "                Args:\n",
    "                    x (torch.Tensor): Temporal input data [batch_size, seq_len, input_size].\n",
    "                    m (torch.Tensor): Masking vector [batch_size, seq_len, input_size].\n",
    "                    delta (torch.Tensor): Time intervals [batch_size, seq_len, input_size].\n",
    "                    static_data (torch.Tensor): Static data input [batch_size, static_input_size].\n",
    "                \"\"\"\n",
    "                batch_size, seq_len, _ = x.size()\n",
    "                \n",
    "                # Process static data to generate attention weights\n",
    "                static_embed = torch.relu(self.static_fc(static_data))  # [batch_size, static_embedding_size]\n",
    "                \n",
    "                # Generate attention weights from static data\n",
    "                attention_weights = torch.sigmoid(self.attention(static_embed))  # [batch_size, hidden_size]\n",
    "                \n",
    "                # Initialize hidden state\n",
    "                h = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "                \n",
    "                for t in range(seq_len):\n",
    "                    # Corrected input decay mechanism\n",
    "                    gamma_x_t = torch.exp(-F.relu(self.gamma_x(delta[:, t, :])))\n",
    "                    x_t_hat = m[:, t, :] * x[:, t, :] + (1 - m[:, t, :]) * ((1 - gamma_x_t) * self.mean_values + gamma_x_t * x[:, t, :])\n",
    "                    \n",
    "                    # Concatenate input, hidden state, and mask\n",
    "                    combined = torch.cat([x_t_hat, h, m[:, t, :]], dim=-1)\n",
    "        \n",
    "                    # GRU gates\n",
    "                    r_t = torch.sigmoid(self.rl(combined))  # Reset gate\n",
    "                    z_t = torch.sigmoid(self.zl(combined))  # Update gate\n",
    "                    h_tilde = torch.tanh(self.hl(torch.cat([x_t_hat, r_t * h, m[:, t, :]], dim=-1)))  # Candidate hidden state\n",
    "        \n",
    "                    # Update hidden state\n",
    "                    h = (1 - z_t) * h + z_t * h_tilde\n",
    "        \n",
    "                # Apply attention weights to the final hidden state\n",
    "                h_weighted = attention_weights * h  # [batch_size, hidden_size]\n",
    "\n",
    "                h_concat = self.dropout(torch.cat([h_weighted, static_embed], dim=-1))  # [B, hidden + static_embedding_size]\n",
    "            \n",
    "                output = torch.sigmoid(self.concat_fc(h_concat)) \n",
    "                return output\n",
    "        \n",
    "        # Training Parameters\n",
    "        input_size = X_temporal_keras.shape[2]  # Temporal input features\n",
    "        hidden_size = 32  # GRU hidden state size\n",
    "        output_size = 1  # Binary classification\n",
    "        static_input_size = X_static_keras.shape[1]  # Static input features\n",
    "        static_embedding_size = 32  # Embedding size for static features\n",
    "        mean_values = torch.tensor(np.nanmean(X_temporal_keras, axis=(0, 1)), dtype=torch.float32)  # Temporal feature means\n",
    "\n",
    "        # Convert data to PyTorch tensors\n",
    "        x = torch.tensor(X_temporal_keras, dtype=torch.float32)  # Temporal input\n",
    "        mask = torch.tensor(X_temporal_mask_keras, dtype=torch.float32)  # Masking vector\n",
    "        delta = torch.tensor(X_temporal_delta_keras, dtype=torch.float32)  # Time intervals\n",
    "        static_data = torch.tensor(X_static_keras, dtype=torch.float32)  # Static input\n",
    "        y = torch.tensor(y_keras, dtype=torch.float32)  # Binary labels\n",
    "        \n",
    "        def GRUD_Static_Model_trained(model_class, x, mask, delta, static_data, y, n_epochs, learning_rate):\n",
    "            model = model_class(\n",
    "                input_size=x.shape[2],\n",
    "                hidden_size=hidden_size,\n",
    "                output_size=output_size,\n",
    "                mean_values=mean_values,\n",
    "                static_input_size=static_input_size,\n",
    "                static_embedding_size=static_embedding_size,\n",
    "            )\n",
    "        \n",
    "            criterion = nn.BCELoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "            for epoch in range(n_epochs):\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(x, mask, delta, static_data)\n",
    "                loss = criterion(outputs.squeeze(), y.squeeze())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "            model.eval()\n",
    "            return model\n",
    "\n",
    "        trained_model = GRUD_Static_Model_trained(\n",
    "            GRUDWithStaticAttention,\n",
    "            x,\n",
    "            mask,\n",
    "            delta,\n",
    "            static_data,\n",
    "            y,\n",
    "            n_epochs=100,\n",
    "            learning_rate=0.01\n",
    "        )\n",
    "        \n",
    "        # Convert test data to torch tensors\n",
    "        x_test = torch.tensor(X_test_temporal_keras, dtype=torch.float32)\n",
    "        mask_test = torch.tensor(X_test_temporal_mask_keras, dtype=torch.float32)\n",
    "        delta_test = torch.tensor(X_test_temporal_delta_keras, dtype=torch.float32)\n",
    "        static_test = torch.tensor(X_test_static_keras, dtype=torch.float32)\n",
    "        y_test_true = y_test_keras.astype(int)  # Convert to 0/1 labels\n",
    "\n",
    "        x_target_sub = torch.tensor(X_target_temporal_keras_sub, dtype=torch.float32)\n",
    "        mask_target_sub = torch.tensor(X_target_temporal_mask_keras_sub, dtype=torch.float32)\n",
    "        delta_target_sub = torch.tensor(X_target_temporal_delta_keras_sub, dtype=torch.float32)\n",
    "        static_target_sub = torch.tensor(X_target_static_keras_sub, dtype=torch.float32)\n",
    "        y_target_soft_sub = torch.tensor(y_target_soft_keras_sub, dtype=torch.float32)\n",
    "\n",
    "        def fine_tune_model_freeze(\n",
    "            model, x, mask, delta, static_data, y_soft,\n",
    "            n_epochs, learning_rate,\n",
    "            patience, min_delta, restore_best=True,\n",
    "            verbose=False\n",
    "        ):\n",
    "            # Freeze all layers\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "            # Unfreeze attention and final output layers\n",
    "            for name, param in model.named_parameters():\n",
    "                if ('attention' in name) or ('fc' in name):\n",
    "                    param.requires_grad = True\n",
    "        \n",
    "            model.train()\n",
    "            optimizer = torch.optim.Adam(\n",
    "                (p for p in model.parameters() if p.requires_grad),\n",
    "                lr=learning_rate\n",
    "            )\n",
    "            criterion = nn.BCELoss()\n",
    "        \n",
    "            best_loss = float(\"inf\")\n",
    "            best_state = None\n",
    "            epochs_no_improve = 0\n",
    "        \n",
    "            for epoch in range(n_epochs):\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "                outputs = model(x, mask, delta, static_data).squeeze()\n",
    "                loss = criterion(outputs, y_soft)\n",
    "        \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "                current_loss = float(loss.item())\n",
    "        \n",
    "                # Early stopping check\n",
    "                improved = (best_loss - current_loss) > min_delta\n",
    "                if improved:\n",
    "                    best_loss = current_loss\n",
    "                    epochs_no_improve = 0\n",
    "                    if restore_best:\n",
    "                        best_state = copy.deepcopy(model.state_dict())\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "        \n",
    "                if epochs_no_improve >= patience:\n",
    "                    if verbose:\n",
    "                        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "        \n",
    "            if restore_best and best_state is not None:\n",
    "                model.load_state_dict(best_state)\n",
    "        \n",
    "            model.eval()\n",
    "            return model\n",
    "            \n",
    "        fine_tuned_model = fine_tune_model_freeze(\n",
    "            trained_model,\n",
    "            x_target_sub, mask_target_sub, delta_target_sub, static_target_sub,\n",
    "            y_target_soft_sub, \n",
    "            n_epochs=1000,          # max epochs (upper bound)\n",
    "            learning_rate=0.01,\n",
    "            patience=10,           # stop if no improvement for 5 epochs\n",
    "            min_delta=1e-4,       # minimum loss improvement\n",
    "            restore_best=True,    # reload best weights\n",
    "            verbose=True          # optional logging\n",
    "        )\n",
    "\n",
    "       # Evaluate fine-tuned model on test set\n",
    "        fine_tuned_model.eval()\n",
    "        with torch.no_grad():\n",
    "            probs = fine_tuned_model(x_test, mask_test, delta_test, static_test).squeeze().cpu().numpy()\n",
    "        \n",
    "        def evaluate_predictions(y_true, y_prob, threshold=threshold):\n",
    "            y_pred = (y_prob > threshold).astype(int)\n",
    "            return {\n",
    "                \"Outcome\": outcome,\n",
    "                \"Precision\": precision_score(y_true, y_pred, zero_division=0).round(3),\n",
    "                \"Recall\": recall_score(y_true, y_pred, zero_division=0).round(3),\n",
    "                \"F1 Score\": f1_score(y_true, y_pred, zero_division=0).round(3),\n",
    "                \"ROC-AUC\": roc_auc_score(y_true, y_prob).round(3),\n",
    "                \"threshold\": threshold.round(3)\n",
    "            }\n",
    "        \n",
    "        # Evaluate and print results\n",
    "        metrics = evaluate_predictions(y_test_true, probs, threshold)\n",
    "        \n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "397c6f82-b70b-4165-afda-40ee1500964b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 67\n",
      "{'Outcome': 'STILLBIRTH_SIGNS_LIFE', 'Precision': 0.173, 'Recall': 0.703, 'F1 Score': 0.278, 'ROC-AUC': 0.859, 'threshold': 0.026}\n",
      "Early stopping at epoch 45\n",
      "{'Outcome': 'LBW2500_ANY', 'Precision': 0.464, 'Recall': 0.575, 'F1 Score': 0.514, 'ROC-AUC': 0.835, 'threshold': 0.142}\n",
      "Early stopping at epoch 86\n",
      "{'Outcome': 'SVN', 'Precision': 0.317, 'Recall': 0.614, 'F1 Score': 0.419, 'ROC-AUC': 0.655, 'threshold': 0.239}\n",
      "Early stopping at epoch 79\n",
      "{'Outcome': 'NEARMISS', 'Precision': 0.114, 'Recall': 0.53, 'F1 Score': 0.188, 'ROC-AUC': 0.606, 'threshold': 0.083}\n",
      "Early stopping at epoch 46\n",
      "{'Outcome': 'NEO_DTH', 'Precision': 0.044, 'Recall': 0.9, 'F1 Score': 0.084, 'ROC-AUC': 0.92, 'threshold': 0.006}\n"
     ]
    }
   ],
   "source": [
    "outcomes = [\"STILLBIRTH_SIGNS_LIFE\",\"LBW2500_ANY\", \"SVN\", \"NEARMISS\", \"NEO_DTH\"]\n",
    "probs = ['stillbirth.prob', 'lbw.prob', 'svn.prob', 'nearmiss.prob','neo_dth.prob']\n",
    "\n",
    "# Function to process results for an outcome\n",
    "def process_results(outcome,prob):\n",
    "    # Run the function and capture the returned values\n",
    "    results_summary= print_results(outcome,prob,0.05)\n",
    "    print(results_summary)\n",
    "    \n",
    "# Run the function for each outcome\n",
    "for outcome, prob in zip(outcomes, probs):\n",
    "    process_results(outcome,prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cc9df86-e279-48eb-b29c-5901221d7e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 83\n",
      "{'Outcome': 'STILLBIRTH_SIGNS_LIFE', 'Precision': 0.204, 'Recall': 0.784, 'F1 Score': 0.324, 'ROC-AUC': 0.906, 'threshold': 0.026}\n",
      "Early stopping at epoch 82\n",
      "{'Outcome': 'LBW2500_ANY', 'Precision': 0.425, 'Recall': 0.684, 'F1 Score': 0.524, 'ROC-AUC': 0.842, 'threshold': 0.142}\n",
      "Early stopping at epoch 78\n",
      "{'Outcome': 'SVN', 'Precision': 0.307, 'Recall': 0.687, 'F1 Score': 0.424, 'ROC-AUC': 0.655, 'threshold': 0.239}\n",
      "Early stopping at epoch 93\n",
      "{'Outcome': 'NEARMISS', 'Precision': 0.112, 'Recall': 0.515, 'F1 Score': 0.184, 'ROC-AUC': 0.603, 'threshold': 0.083}\n",
      "Early stopping at epoch 42\n",
      "{'Outcome': 'NEO_DTH', 'Precision': 0.036, 'Recall': 0.7, 'F1 Score': 0.069, 'ROC-AUC': 0.919, 'threshold': 0.006}\n"
     ]
    }
   ],
   "source": [
    "outcomes = [\"STILLBIRTH_SIGNS_LIFE\",\"LBW2500_ANY\", \"SVN\", \"NEARMISS\", \"NEO_DTH\"]\n",
    "probs = ['stillbirth.prob', 'lbw.prob', 'svn.prob', 'nearmiss.prob','neo_dth.prob']\n",
    "\n",
    "# Function to process results for an outcome\n",
    "def process_results(outcome,prob):\n",
    "    # Run the function and capture the returned values\n",
    "    results_summary= print_results(outcome,prob,0.1)\n",
    "    print(results_summary)\n",
    "    \n",
    "# Run the function for each outcome\n",
    "for outcome, prob in zip(outcomes, probs):\n",
    "    process_results(outcome,prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5f858a4-1e02-4647-9008-eadf2e6bd68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 74\n",
      "{'Outcome': 'STILLBIRTH_SIGNS_LIFE', 'Precision': 0.265, 'Recall': 0.811, 'F1 Score': 0.4, 'ROC-AUC': 0.923, 'threshold': 0.026}\n",
      "Early stopping at epoch 70\n",
      "{'Outcome': 'LBW2500_ANY', 'Precision': 0.431, 'Recall': 0.646, 'F1 Score': 0.517, 'ROC-AUC': 0.835, 'threshold': 0.142}\n",
      "Early stopping at epoch 68\n",
      "{'Outcome': 'SVN', 'Precision': 0.302, 'Recall': 0.661, 'F1 Score': 0.415, 'ROC-AUC': 0.652, 'threshold': 0.239}\n",
      "Early stopping at epoch 121\n",
      "{'Outcome': 'NEARMISS', 'Precision': 0.118, 'Recall': 0.485, 'F1 Score': 0.19, 'ROC-AUC': 0.598, 'threshold': 0.083}\n",
      "Early stopping at epoch 42\n",
      "{'Outcome': 'NEO_DTH', 'Precision': 0.035, 'Recall': 0.7, 'F1 Score': 0.066, 'ROC-AUC': 0.919, 'threshold': 0.006}\n"
     ]
    }
   ],
   "source": [
    "outcomes = [\"STILLBIRTH_SIGNS_LIFE\",\"LBW2500_ANY\", \"SVN\", \"NEARMISS\", \"NEO_DTH\"]\n",
    "probs = ['stillbirth.prob', 'lbw.prob', 'svn.prob', 'nearmiss.prob','neo_dth.prob']\n",
    "\n",
    "# Function to process results for an outcome\n",
    "def process_results(outcome,prob):\n",
    "    # Run the function and capture the returned values\n",
    "    results_summary= print_results(outcome,prob,0.2)\n",
    "    print(results_summary)\n",
    "    \n",
    "# Run the function for each outcome\n",
    "for outcome, prob in zip(outcomes, probs):\n",
    "    process_results(outcome,prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bd8f8d4-f42b-434b-a16e-447dbae0ec18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 110\n",
      "{'Outcome': 'STILLBIRTH_SIGNS_LIFE', 'Precision': 0.265, 'Recall': 0.811, 'F1 Score': 0.4, 'ROC-AUC': 0.936, 'threshold': 0.026}\n",
      "Early stopping at epoch 132\n",
      "{'Outcome': 'LBW2500_ANY', 'Precision': 0.386, 'Recall': 0.726, 'F1 Score': 0.504, 'ROC-AUC': 0.843, 'threshold': 0.142}\n",
      "Early stopping at epoch 158\n",
      "{'Outcome': 'SVN', 'Precision': 0.307, 'Recall': 0.69, 'F1 Score': 0.425, 'ROC-AUC': 0.659, 'threshold': 0.239}\n",
      "Early stopping at epoch 128\n",
      "{'Outcome': 'NEARMISS', 'Precision': 0.108, 'Recall': 0.53, 'F1 Score': 0.18, 'ROC-AUC': 0.596, 'threshold': 0.083}\n",
      "Early stopping at epoch 76\n",
      "{'Outcome': 'NEO_DTH', 'Precision': 0.041, 'Recall': 0.8, 'F1 Score': 0.078, 'ROC-AUC': 0.909, 'threshold': 0.006}\n"
     ]
    }
   ],
   "source": [
    "outcomes = [\"STILLBIRTH_SIGNS_LIFE\",\"LBW2500_ANY\", \"SVN\", \"NEARMISS\", \"NEO_DTH\"]\n",
    "probs = ['stillbirth.prob', 'lbw.prob', 'svn.prob', 'nearmiss.prob','neo_dth.prob']\n",
    "\n",
    "# Function to process results for an outcome\n",
    "def process_results(outcome,prob):\n",
    "    # Run the function and capture the returned values\n",
    "    results_summary= print_results(outcome,prob,0.5)\n",
    "    print(results_summary)\n",
    "    \n",
    "# Run the function for each outcome\n",
    "for outcome, prob in zip(outcomes, probs):\n",
    "    process_results(outcome,prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09f65e10-75a0-47b1-8262-30ebca59e7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 82\n",
      "{'Outcome': 'STILLBIRTH_SIGNS_LIFE', 'Precision': 0.252, 'Recall': 0.784, 'F1 Score': 0.382, 'ROC-AUC': 0.938, 'threshold': 0.026}\n",
      "Early stopping at epoch 82\n",
      "{'Outcome': 'LBW2500_ANY', 'Precision': 0.43, 'Recall': 0.665, 'F1 Score': 0.522, 'ROC-AUC': 0.84, 'threshold': 0.142}\n",
      "Early stopping at epoch 120\n",
      "{'Outcome': 'SVN', 'Precision': 0.31, 'Recall': 0.693, 'F1 Score': 0.429, 'ROC-AUC': 0.658, 'threshold': 0.239}\n",
      "Early stopping at epoch 143\n",
      "{'Outcome': 'NEARMISS', 'Precision': 0.111, 'Recall': 0.515, 'F1 Score': 0.183, 'ROC-AUC': 0.602, 'threshold': 0.083}\n",
      "Early stopping at epoch 113\n",
      "{'Outcome': 'NEO_DTH', 'Precision': 0.037, 'Recall': 0.8, 'F1 Score': 0.07, 'ROC-AUC': 0.887, 'threshold': 0.006}\n"
     ]
    }
   ],
   "source": [
    "outcomes = [\"STILLBIRTH_SIGNS_LIFE\",\"LBW2500_ANY\", \"SVN\", \"NEARMISS\", \"NEO_DTH\"]\n",
    "probs = ['stillbirth.prob', 'lbw.prob', 'svn.prob', 'nearmiss.prob','neo_dth.prob']\n",
    "\n",
    "# Function to process results for an outcome\n",
    "def process_results(outcome,prob):\n",
    "    # Run the function and capture the returned values\n",
    "    results_summary= print_results(outcome,prob,1)\n",
    "    print(results_summary)\n",
    "    \n",
    "# Run the function for each outcome\n",
    "for outcome, prob in zip(outcomes, probs):\n",
    "    process_results(outcome,prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e2a790-84c7-47f8-9cc6-7a4c91b76456",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
