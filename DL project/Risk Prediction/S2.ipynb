{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f25fcab8-0a2b-4bf7-afdf-17da6ddea4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, GRU, Input, Multiply, TimeDistributed, Softmax, Lambda, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from xgboost import XGBClassifier\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "        \n",
    "\n",
    "def print_results(outcome_variable):\n",
    "        ### Deep Learning analysis data preparation\n",
    "        # Load data from a CSV file\n",
    "        df = pd.read_csv('D:/Users/yipeng_wei/Documents/dl data/2025-10-31/df_dl_continuous.csv')\n",
    "        df_mask = pd.read_csv('D:/Users/yipeng_wei/Documents/dl data/2025-10-31/df_dl_continuous_mask.csv')\n",
    "        df_delta = pd.read_csv('D:/Users/yipeng_wei/Documents/dl data/2025-10-31/df_dl_continuous_delta.csv')\n",
    "        \n",
    "        # Drop rows with NaN in the target column before any operations\n",
    "        df = df.dropna(subset=[outcome_variable])\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "        df_mask = df_mask.dropna(subset=[outcome_variable])\n",
    "        df_mask = df_mask.reset_index(drop=True)\n",
    "        \n",
    "        df_delta = df_delta.dropna(subset=[outcome_variable])\n",
    "        df_delta = df_delta.reset_index(drop=True)\n",
    "        \n",
    "        features_categorical = [\"TYPE_VISIT\",\"SITE\",\n",
    "                    \"STILLBIRTH_IND\",\"PRETERM_IND\",\"CESARIAN_IND\",\"NUM_MISCARRIAGE_ind\",\n",
    "                    \"DEPR_EVER\",\n",
    "                    \"PAID_WORK\",\"GPARITY_2\",\"GPARITY_1\",\"WEALTH_QUINT_1\",\"WEALTH_QUINT_2\",\"WEALTH_QUINT_3\",\"WEALTH_QUINT_4\",\"SCHOOL_MORE10\",\"WATER_IMPROVED\",\"TOILET_IMPROVED\",\"STOVE_FUEL\",\"HH_SMOKE\",\"SMOKE\",\"CHEW_TOBACCO\",\"CHEW_BETELNUT\",\n",
    "                    \"BMI_LEVEL_ENROLL_underweight\",\"BMI_LEVEL_ENROLL_overweight\",\"BMI_LEVEL_ENROLL_obese\",\n",
    "                    \"GWG_ADEQUACY_inadequate\",\"GWG_ADEQUACY_excessive\",\n",
    "                    \"MAT_CES_ANY\",\"BIRTH_FACILITY\",\"MAT_PRETERM_ANY\",\n",
    "                    \"HTN\",\"DIAB_OVERT_ANY\",\"HIV_POSITIVE\",\"TB_SYMP_POSITIVE\",\"MAL_POSITIVE\",\"HBV_POSITIVE_ENROLL\",\"HCV_POSITIVE_ENROLL\",\"STI_POSITIVE\",\n",
    "                    \"GES_HTN\",\"DIAB_GEST_ANY\",\n",
    "                    \"INF_SEX\",\"INF_HYPERBILI_NICE\",\"BREASTFED\",\n",
    "                    \"NUM_FETUS_2\",\"NUM_FETUS_3\",\"INF_ANOMALY\",\"PREVIA\",\n",
    "                    \"INF_PSBI_ANY\",\n",
    "                    \"M08_ANEMIA\"]\n",
    "        \n",
    "        features_continuous = [\"MAT_AGE\",\"MUAC_ENROLL\",\"BMI_ENROLL\",\"GESTAGEBIRTH_ANY\",\"BWEIGHT_ANY\",\"CROWDING_IND\",\n",
    "                              \"M01_AFI_PERES\",\n",
    "                              \"M08_IRON_TOT_UGDL_LBORRES\", \"M08_FERRITIN_LBORRES\",\"M08_IRON_HEP_LBORRES\",\"M08_CBC_HB_LBORRES\"]\n",
    "        \n",
    "        features_static = [\"SITE\",\n",
    "                    \"STILLBIRTH_IND\",\"PRETERM_IND\",\"CESARIAN_IND\",\"NUM_MISCARRIAGE_ind\",\n",
    "                    \"DEPR_EVER\",\n",
    "                    \"PAID_WORK\",\"GPARITY_2\",\"GPARITY_1\",\"WEALTH_QUINT_1\",\"WEALTH_QUINT_2\",\"WEALTH_QUINT_3\",\"WEALTH_QUINT_4\",\"SCHOOL_MORE10\",\"WATER_IMPROVED\",\"TOILET_IMPROVED\",\"STOVE_FUEL\",\"HH_SMOKE\",\"SMOKE\",\"CHEW_TOBACCO\",\"CHEW_BETELNUT\",\"CROWDING_IND\",\n",
    "                    \"BMI_LEVEL_ENROLL_underweight\",\"BMI_LEVEL_ENROLL_overweight\",\"BMI_LEVEL_ENROLL_obese\",\n",
    "                    \"GWG_ADEQUACY_inadequate\",\"GWG_ADEQUACY_excessive\",\n",
    "                    \"MAT_CES_ANY\",\"BIRTH_FACILITY\",\"MAT_PRETERM_ANY\",\n",
    "                    \"HTN\",\"DIAB_OVERT_ANY\",\"HIV_POSITIVE\",\"TB_SYMP_POSITIVE\",\"MAL_POSITIVE\",\"HBV_POSITIVE_ENROLL\",\"HCV_POSITIVE_ENROLL\",\"STI_POSITIVE\",\n",
    "                    \"GES_HTN\",\"DIAB_GEST_ANY\",\n",
    "                    \"INF_SEX\",\"INF_HYPERBILI_NICE\",\"BREASTFED\",\"GESTAGEBIRTH_ANY\",\"BWEIGHT_ANY\",\n",
    "                    \"INF_ANOMALY\",\n",
    "                    \"INF_PSBI_ANY\",\n",
    "                    \"NUM_FETUS_2\",\"NUM_FETUS_3\",\"MAT_AGE\",\"MUAC_ENROLL\",\n",
    "                    \"BMI_ENROLL\"]\n",
    "        \n",
    "        features_temporal = [\"TYPE_VISIT\",\n",
    "                             \"M01_AFI_PERES\",\n",
    "                             \"PREVIA\",\n",
    "                             \"M08_IRON_TOT_UGDL_LBORRES\",\"M08_FERRITIN_LBORRES\",\"M08_IRON_HEP_LBORRES\",\"M08_CBC_HB_LBORRES\",\"M08_ANEMIA\"]\n",
    "\n",
    "        features_ml = [\"SITE\",\n",
    "                    \"STILLBIRTH_IND\",\"PRETERM_IND\",\"CESARIAN_IND\",\"NUM_MISCARRIAGE_ind\",\n",
    "                    \"DEPR_EVER\",\n",
    "                    \"PAID_WORK\",\"GPARITY_2\",\"GPARITY_1\",\"WEALTH_QUINT_1\",\"WEALTH_QUINT_2\",\"WEALTH_QUINT_3\",\"WEALTH_QUINT_4\",\"SCHOOL_MORE10\",\"WATER_IMPROVED\",\"TOILET_IMPROVED\",\"STOVE_FUEL\",\"HH_SMOKE\",\"SMOKE\",\"CHEW_TOBACCO\",\"CHEW_BETELNUT\",\"CROWDING_IND\",\n",
    "                    \"BMI_LEVEL_ENROLL_underweight\",\"BMI_LEVEL_ENROLL_overweight\",\"BMI_LEVEL_ENROLL_obese\",\n",
    "                    \"GWG_ADEQUACY_inadequate\",\"GWG_ADEQUACY_excessive\",\n",
    "                    \"MAT_CES_ANY\",\"BIRTH_FACILITY\",\"MAT_PRETERM_ANY\",\n",
    "                    \"HTN\",\"DIAB_OVERT_ANY\",\"HIV_POSITIVE\",\"TB_SYMP_POSITIVE\",\"MAL_POSITIVE\",\"HBV_POSITIVE_ENROLL\",\"HCV_POSITIVE_ENROLL\",\"STI_POSITIVE\",\n",
    "                    \"GES_HTN\",\"DIAB_GEST_ANY\",\n",
    "                    \"INF_SEX\",\"INF_HYPERBILI_NICE\",\"BREASTFED\",\"GESTAGEBIRTH_ANY\",\"BWEIGHT_ANY\",\n",
    "                    \"INF_ANOMALY\",\n",
    "                    \"INF_PSBI_ANY\",\n",
    "                    \"NUM_FETUS_2\",\"NUM_FETUS_3\",\"MAT_AGE\",\"MUAC_ENROLL\",\n",
    "                    \"BMI_ENROLL\",\n",
    "                    \"M01_AFI_PERES_1\",\"M01_AFI_PERES_2\",\"M01_AFI_PERES_3\",\"M01_AFI_PERES_4\",\"M01_AFI_PERES_5\",\n",
    "                    \"PREVIA_1\",\"PREVIA_2\",\"PREVIA_3\",\"PREVIA_4\",\"PREVIA_5\",\n",
    "                    \"M08_FERRITIN_LBORRES_1\", \"M08_FERRITIN_LBORRES_2\", \"M08_FERRITIN_LBORRES_3\", \"M08_FERRITIN_LBORRES_4\", \"M08_FERRITIN_LBORRES_5\",\n",
    "                    \"M08_IRON_TOT_UGDL_LBORRES_1\", \"M08_IRON_TOT_UGDL_LBORRES_2\", \"M08_IRON_TOT_UGDL_LBORRES_3\", \"M08_IRON_TOT_UGDL_LBORRES_4\", \"M08_IRON_TOT_UGDL_LBORRES_5\",\n",
    "                    \"M08_IRON_HEP_LBORRES_1\", \"M08_IRON_HEP_LBORRES_2\", \"M08_IRON_HEP_LBORRES_3\", \"M08_IRON_HEP_LBORRES_4\", \"M08_IRON_HEP_LBORRES_5\",\n",
    "                    \"M08_CBC_HB_LBORRES_1\", \"M08_CBC_HB_LBORRES_2\", \"M08_CBC_HB_LBORRES_3\", \"M08_CBC_HB_LBORRES_4\", \"M08_CBC_HB_LBORRES_5\",\n",
    "                    \"M08_ANEMIA_1\", \"M08_ANEMIA_2\", \"M08_ANEMIA_3\", \"M08_ANEMIA_4\", \"M08_ANEMIA_5\"]\n",
    "\n",
    "        features_to_remove_by_outcome = {\n",
    "        \"STILLBIRTH_SIGNS_LIFE\": [\"INF_HYPERBILI_NICE\",\"BREASTFED\",\"GESTAGEBIRTH_ANY\",\"MAT_PRETERM_ANY\",\"BWEIGHT_ANY\",\"INF_PSBI_ANY\"],\n",
    "        \"PRETERMBIRTH_LT37\": [\"INF_HYPERBILI_NICE\",\"BREASTFED\",\"GESTAGEBIRTH_ANY\",\"MAT_PRETERM_ANY\",\"BWEIGHT_ANY\",\"INF_PSBI_ANY\",\"MAT_CES_ANY\",\"BIRTH_FACILITY\"],\n",
    "        \"LBW2500_ANY\": [\"INF_HYPERBILI_NICE\",\"BREASTFED\",\"BWEIGHT_ANY\",\"INF_PSBI_ANY\",\"MAT_CES_ANY\",\"BIRTH_FACILITY\"],\n",
    "        \"INF_PSBI_OUTCOME\": [\"INF_HYPERBILI_NICE\",\"INF_PSBI_ANY\",\"PREVIA\",\"PREVIA_1\",\"PREVIA_2\",\"PREVIA_3\",\"PREVIA_4\",\"PREVIA_5\"],\n",
    "        \"SVN\": [\"INF_SEX\",\"INF_HYPERBILI_NICE\",\"GESTAGEBIRTH_ANY\",\"MAT_PRETERM_ANY\",\"BWEIGHT_ANY\",\"INF_PSBI_ANY\",\"BREASTFED\",\"MAT_CES_ANY\",\"BIRTH_FACILITY\"],\n",
    "        \"NEARMISS\": [\"INF_SEX\",\"INF_HYPERBILI_NICE\",\"BREASTFED\",\"GESTAGEBIRTH_ANY\",\"MAT_PRETERM_ANY\",\"BWEIGHT_ANY\",\"INF_PSBI_ANY\"]\n",
    "        }\n",
    "    \n",
    "        features_to_remove_by_condition = [\"HIV_POSITIVE\",\"TB_SYMP_POSITIVE\",\"MAL_POSITIVE\",\"HBV_POSITIVE_ENROLL\",\"HCV_POSITIVE_ENROLL\",\"STI_POSITIVE\",\n",
    "                                           \"M08_IRON_TOT_UGDL_LBORRES\",\"M08_FERRITIN_LBORRES\",\"M08_IRON_HEP_LBORRES\", \"M08_CBC_HB_LBORRES\", \"M08_ANEMIA\",\n",
    "                                           \"M08_FERRITIN_LBORRES_1\", \"M08_FERRITIN_LBORRES_2\", \"M08_FERRITIN_LBORRES_3\", \"M08_FERRITIN_LBORRES_4\", \"M08_FERRITIN_LBORRES_5\",\n",
    "                                           \"M08_IRON_TOT_UGDL_LBORRES_1\", \"M08_IRON_TOT_UGDL_LBORRES_2\", \"M08_IRON_TOT_UGDL_LBORRES_3\", \"M08_IRON_TOT_UGDL_LBORRES_4\", \"M08_IRON_TOT_UGDL_LBORRES_5\",\n",
    "                                           \"M08_IRON_HEP_LBORRES_1\", \"M08_IRON_HEP_LBORRES_2\", \"M08_IRON_HEP_LBORRES_3\", \"M08_IRON_HEP_LBORRES_4\", \"M08_IRON_HEP_LBORRES_5\",\n",
    "                                           \"M08_CBC_HB_LBORRES_1\",\"M08_CBC_HB_LBORRES_2\",\"M08_CBC_HB_LBORRES_3\",\"M08_CBC_HB_LBORRES_4\",\"M08_CBC_HB_LBORRES_5\",\n",
    "                                           \"M08_ANEMIA_1\", \"M08_ANEMIA_2\", \"M08_ANEMIA_3\", \"M08_ANEMIA_4\", \"M08_ANEMIA_5\"]\n",
    "    \n",
    "        features_to_remove_pre = features_to_remove_by_outcome.get(outcome_variable, [])\n",
    "        features_to_remove = list(set(features_to_remove_pre + features_to_remove_by_condition))\n",
    "        \n",
    "        # Remove features from the lists\n",
    "        features_categorical = [f for f in features_categorical if f not in features_to_remove]\n",
    "        features_continuous = [f for f in features_continuous if f not in features_to_remove]\n",
    "        features_static = [f for f in features_static if f not in features_to_remove]\n",
    "        features_temporal = [f for f in features_temporal if f not in features_to_remove]\n",
    "        features_ml = [f for f in features_ml if f not in features_to_remove]\n",
    "        \n",
    "        # Separate categorical and continuous features\n",
    "        df_categorical = df[features_categorical]\n",
    "        df_continuous = df[features_continuous]\n",
    "        \n",
    "        lower_q = 0.02   # 2nd percentile\n",
    "        upper_q = 0.98   # 98th percentile\n",
    "        \n",
    "        df_continuous_capped = df_continuous.copy()\n",
    "        \n",
    "        for col in features_continuous:\n",
    "            lower = df_continuous[col].quantile(lower_q)\n",
    "            upper = df_continuous[col].quantile(upper_q)\n",
    "            df_continuous_capped[col] = df_continuous[col].clip(lower, upper)\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        df_continuous_scaled = pd.DataFrame(\n",
    "            scaler.fit_transform(df_continuous_capped),\n",
    "            columns=features_continuous,\n",
    "            index=df.index\n",
    "        )\n",
    "        \n",
    "        # Concatenate scaled continuous features with categorical features\n",
    "        X = pd.concat([df_categorical, df_continuous_scaled], axis=1)\n",
    "\n",
    "        ##Static and temporal data\n",
    "        X_static = X[features_static]\n",
    "        \n",
    "        X_temporal = X[features_temporal]\n",
    "        X_temporal_delta = df_delta[features_temporal]\n",
    "        X_temporal_mask = df_mask[features_temporal]\n",
    "        \n",
    "        y = df[outcome_variable]\n",
    "        threshold = (df[outcome_variable] == 1).mean()\n",
    "        \n",
    "        # One-hot encoding for non-binary categorical features in X\n",
    "        X = pd.get_dummies(X, columns=['TYPE_VISIT','SITE'], dummy_na=False)\n",
    "        X_static = pd.get_dummies(X_static, columns=['SITE'], dummy_na=False)\n",
    "        X_temporal = pd.get_dummies(X_temporal, columns=['TYPE_VISIT'], dummy_na=False)\n",
    "        X_temporal_delta = pd.get_dummies(X_temporal_delta, columns=['TYPE_VISIT'], dummy_na=False)\n",
    "        X_temporal_mask = pd.get_dummies(X_temporal_mask, columns=['TYPE_VISIT'], dummy_na=False)\n",
    "        \n",
    "        # Preprocess for Keras models (3D matrix)\n",
    "        n_samples = X_temporal.shape[0] // 5 \n",
    "        X_static_keras = X_static.iloc[::5].values.astype('float32')\n",
    "        \n",
    "        X_temporal_keras = X_temporal.values.reshape(n_samples, 5, X_temporal.shape[1]).astype('float32')\n",
    "        X_temporal_delta_keras = X_temporal_delta.values.reshape(n_samples, 5, X_temporal_delta.shape[1]).astype('float32')\n",
    "        X_temporal_mask_keras = X_temporal_mask.values.reshape(n_samples, 5, X_temporal_mask.shape[1]).astype('float32')\n",
    "        \n",
    "        X_keras = X.values.reshape(n_samples, 5, X.shape[1]).astype('float32')\n",
    "        \n",
    "        # Align y for Keras models\n",
    "        y_keras = y.iloc[::5].values.astype('float32')\n",
    "        \n",
    "        ### Machine Learning analysis data preparation\n",
    "        # Load data from a CSV file\n",
    "        ml = pd.read_csv('D:/Users/yipeng_wei/Documents/dl data/2025-10-31/df_dl_continuous_wide.csv')\n",
    "        \n",
    "        # Drop rows with NaN in the target column before any operations\n",
    "        ml = ml.dropna(subset=[outcome_variable])\n",
    "        ml = ml.reset_index(drop=True)\n",
    "        threshold = (ml[outcome_variable] == 1).mean()\n",
    "        \n",
    "        # Normalize specified features before imputing missing values\n",
    "        features_to_normalize = [\"MAT_AGE\",\"MUAC_ENROLL\",\"GESTAGEBIRTH_ANY\",\"BWEIGHT_ANY\",\"CROWDING_IND\",\n",
    "                                 \"BMI_ENROLL\",\n",
    "                                 \"M01_AFI_PERES_1\",\"M01_AFI_PERES_2\",\"M01_AFI_PERES_3\",\"M01_AFI_PERES_4\",\"M01_AFI_PERES_5\",\n",
    "                                 \"M08_FERRITIN_LBORRES_1\", \"M08_FERRITIN_LBORRES_2\", \"M08_FERRITIN_LBORRES_3\", \"M08_FERRITIN_LBORRES_4\", \"M08_FERRITIN_LBORRES_5\",\n",
    "                                 \"M08_IRON_TOT_UGDL_LBORRES_1\", \"M08_IRON_TOT_UGDL_LBORRES_2\", \"M08_IRON_TOT_UGDL_LBORRES_3\", \"M08_IRON_TOT_UGDL_LBORRES_4\", \"M08_IRON_TOT_UGDL_LBORRES_5\",\n",
    "                                 \"M08_IRON_HEP_LBORRES_1\", \"M08_IRON_HEP_LBORRES_2\", \"M08_IRON_HEP_LBORRES_3\", \"M08_IRON_HEP_LBORRES_4\", \"M08_IRON_HEP_LBORRES_5\",\n",
    "                                 \"M08_CBC_HB_LBORRES_1\", \"M08_CBC_HB_LBORRES_2\", \"M08_CBC_HB_LBORRES_3\", \"M08_CBC_HB_LBORRES_4\", \"M08_CBC_HB_LBORRES_5\"]\n",
    "        \n",
    "        # Create a MinMaxScaler instance\n",
    "        for col in features_to_normalize:\n",
    "            lower = ml[col].quantile(lower_q)\n",
    "            upper = ml[col].quantile(upper_q)        \n",
    "            ml[col] = ml[col].clip(lower, upper)\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        ml[features_to_normalize] = scaler.fit_transform(ml[features_to_normalize])\n",
    "        \n",
    "        X_ml = ml[features_ml]\n",
    "        y_ml = ml[outcome_variable]\n",
    "        \n",
    "        X_ml = X_ml.fillna(0)\n",
    "        X_ml = pd.get_dummies(X_ml, columns=['SITE'], dummy_na=False)\n",
    "        \n",
    "        # Preprocess for scikit-learn models (2D matrix)\n",
    "        X_sklearn = X_ml.values.astype('float32')\n",
    "        y_sklearn = y_ml.values.astype('float32')\n",
    "        \n",
    "        def evaluate_model(model_func, X, y, model_type=\"sklearn\"):\n",
    "            # 5-fold cross-validation\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            \n",
    "            # Lists to store scores for each fold\n",
    "            precision_scores = []\n",
    "            recall_scores = []\n",
    "            f1_scores = []  # List to store F1 scores\n",
    "            roc_auc_scores = []  # List to store ROC-AUC scores\n",
    "            \n",
    "            # Cross-validation loop\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "                threshold = y_train.mean()\n",
    "                \n",
    "                if model_type == \"sklearn\":\n",
    "                    model = model_func()\n",
    "                    model.fit(X_train, y_train)\n",
    "        \n",
    "                    y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "                \n",
    "                else:  # Keras models: GRU, RETAIN, PredictPTB\n",
    "                    X_train_part, X_val = X_train[:int(0.9 * len(X_train))], X_train[int(0.9 * len(X_train)):]\n",
    "                    y_train_part, y_val = y_train[:int(0.9 * len(y_train))], y_train[int(0.9 * len(y_train)):]\n",
    "                    \n",
    "                    model = model_func(input_shape=(X_train_part.shape[1], X_train_part.shape[2]))\n",
    "                    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "                    \n",
    "                    # Train the Keras model with early stopping\n",
    "                    model.fit(X_train_part, y_train_part, epochs=200, batch_size=256, verbose=0, \n",
    "                              validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "                    \n",
    "                    y_pred_prob = model.predict(X_test)  # Predicted probabilities\n",
    "                \n",
    "                y_pred = (y_pred_prob > threshold).astype('int32')\n",
    "                \n",
    "                # Calculate metrics for this fold\n",
    "                precision = precision_score(y_test, y_pred)\n",
    "                recall = recall_score(y_test, y_pred)\n",
    "                f1 = f1_score(y_test, y_pred)  # Calculate F1 score\n",
    "                roc_auc = roc_auc_score(y_test, y_pred_prob)  # Calculate ROC-AUC score\n",
    "                \n",
    "                # Append scores\n",
    "                precision_scores.append(precision)\n",
    "                recall_scores.append(recall)\n",
    "                f1_scores.append(f1)  # Append F1 score\n",
    "                roc_auc_scores.append(roc_auc)  # Append ROC-AUC score\n",
    "            \n",
    "            # Compute and return mean scores across all folds\n",
    "            mean_precision = np.mean(precision_scores)\n",
    "            mean_recall = np.mean(recall_scores)\n",
    "            mean_f1 = np.mean(f1_scores)  # Mean F1 score\n",
    "            mean_roc_auc = np.mean(roc_auc_scores)  # Mean ROC-AUC score\n",
    "            \n",
    "            return mean_precision, mean_recall, mean_f1, mean_roc_auc\n",
    "        \n",
    "        # 1. Logistic Regression\n",
    "        def logistic_regression():\n",
    "            return LogisticRegression(max_iter=1000, random_state=42)\n",
    "        \n",
    "        # 2. Random Forest\n",
    "        def random_forest():\n",
    "            return RandomForestClassifier(random_state=42)\n",
    "        \n",
    "        # 3. SVM model\n",
    "        def svm_model():\n",
    "            return SVC(probability=True, random_state=42)\n",
    "            \n",
    "        # 4. PredictPTB model\n",
    "        def build_predict_ptb(input_shape):\n",
    "            inputs = Input(shape=input_shape)\n",
    "            visit_embedding = TimeDistributed(Dense(64, activation='relu'))(inputs)\n",
    "            rnn_beta, _ = GRU(32, return_sequences=True, return_state=True)(visit_embedding)\n",
    "            beta = TimeDistributed(Dense(64, activation='tanh'))(rnn_beta)\n",
    "            elementwise_prod = Multiply()([beta, visit_embedding])\n",
    "            context_vector = Lambda(lambda x: tf.reduce_sum(x, axis=1))(elementwise_prod)\n",
    "            output = Dense(1, activation='sigmoid')(context_vector)\n",
    "            model = Model(inputs=inputs, outputs=output)\n",
    "            model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy')\n",
    "            return model\n",
    "        \n",
    "        # 5. RETAIN model\n",
    "        def build_retain(input_shape):\n",
    "            inputs = Input(shape=input_shape)\n",
    "            visit_embedding = TimeDistributed(Dense(64, activation='relu'))(inputs)\n",
    "            rnn_alpha, _ = GRU(32, return_sequences=True, return_state=True)(visit_embedding)\n",
    "            attention_scores = TimeDistributed(Dense(1))(rnn_alpha)\n",
    "            alpha = Softmax(axis=1)(attention_scores)\n",
    "            rnn_beta, _ = GRU(32, return_sequences=True, return_state=True)(visit_embedding)\n",
    "            beta = TimeDistributed(Dense(64, activation='tanh'))(rnn_beta)\n",
    "            weighted_context = Multiply()([alpha, Multiply()([beta, visit_embedding])])\n",
    "            context_vector = Lambda(lambda x: tf.reduce_sum(x, axis=1))(weighted_context)\n",
    "            output = Dense(1, activation='sigmoid')(context_vector)\n",
    "            model = Model(inputs=inputs, outputs=output)\n",
    "            model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy')\n",
    "            return model\n",
    "        \n",
    "        #### 6. GRUD With Static Attention\n",
    "        class GRUDWithStaticAttention(nn.Module):\n",
    "            def __init__(self, input_size, hidden_size, output_size, mean_values, static_input_size, static_embedding_size, dropout_rate=0.5):\n",
    "                \"\"\"\n",
    "                GRU-D model with attention mechanism generated from static data.\n",
    "                \n",
    "                Args:\n",
    "                    input_size (int): Number of input features for temporal data.\n",
    "                    hidden_size (int): Number of hidden units in GRU.\n",
    "                    output_size (int): Number of output classes (for classification tasks).\n",
    "                    mean_values (torch.Tensor): Empirical mean values for each input feature.\n",
    "                    static_input_size (int): Number of input features for static data.\n",
    "                    static_embedding_size (int): Size of the embedding for static data.\n",
    "                \"\"\"\n",
    "                super(GRUDWithStaticAttention, self).__init__()\n",
    "                \n",
    "                # Temporal (GRU-D) parameters\n",
    "                self.input_size = input_size\n",
    "                self.hidden_size = hidden_size\n",
    "                self.output_size = output_size\n",
    "                \n",
    "                self.mean_values = mean_values\n",
    "                \n",
    "                self.gamma_x = nn.Linear(input_size, input_size)\n",
    "                \n",
    "                # GRU gate parameters (temporal part) with combined input, hidden state, and mask\n",
    "                self.zl = nn.Linear(input_size + hidden_size + input_size, hidden_size)  # Update gate\n",
    "                self.rl = nn.Linear(input_size + hidden_size + input_size, hidden_size)  # Reset gate\n",
    "                self.hl = nn.Linear(input_size + hidden_size + input_size, hidden_size)  # Candidate hidden state\n",
    "        \n",
    "                # Embedding or processing static data\n",
    "                self.static_fc = nn.Linear(static_input_size, static_embedding_size)\n",
    "        \n",
    "                # Attention mechanism based on static data\n",
    "                self.attention = nn.Linear(static_embedding_size, hidden_size)\n",
    "        \n",
    "                # Fully connected output layer\n",
    "                self.dropout = nn.Dropout(dropout_rate)\n",
    "                self.concat_fc = nn.Linear(hidden_size + static_embedding_size, output_size)\n",
    "\n",
    "        \n",
    "            def forward(self, x, m, delta, static_data):\n",
    "                \"\"\"\n",
    "                Forward pass of GRU-D with static data attention.\n",
    "                \n",
    "                Args:\n",
    "                    x (torch.Tensor): Temporal input data [batch_size, seq_len, input_size].\n",
    "                    m (torch.Tensor): Masking vector [batch_size, seq_len, input_size].\n",
    "                    delta (torch.Tensor): Time intervals [batch_size, seq_len, input_size].\n",
    "                    static_data (torch.Tensor): Static data input [batch_size, static_input_size].\n",
    "                \"\"\"\n",
    "                batch_size, seq_len, _ = x.size()\n",
    "                \n",
    "                # Process static data to generate attention weights\n",
    "                static_embed = torch.relu(self.static_fc(static_data))  # [batch_size, static_embedding_size]\n",
    "                \n",
    "                # Generate attention weights from static data\n",
    "                attention_weights = torch.sigmoid(self.attention(static_embed))  # [batch_size, hidden_size]\n",
    "                \n",
    "                # Initialize hidden state\n",
    "                h = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "                \n",
    "                for t in range(seq_len):\n",
    "                    # Corrected input decay mechanism\n",
    "                    gamma_x_t = torch.exp(-F.relu(self.gamma_x(delta[:, t, :])))\n",
    "                    x_t_hat = m[:, t, :] * x[:, t, :] + (1 - m[:, t, :]) * ((1 - gamma_x_t) * self.mean_values + gamma_x_t * x[:, t, :])\n",
    "                    \n",
    "                    # Concatenate input, hidden state, and mask\n",
    "                    combined = torch.cat([x_t_hat, h, m[:, t, :]], dim=-1)\n",
    "        \n",
    "                    # GRU gates\n",
    "                    r_t = torch.sigmoid(self.rl(combined))  # Reset gate\n",
    "                    z_t = torch.sigmoid(self.zl(combined))  # Update gate\n",
    "                    h_tilde = torch.tanh(self.hl(torch.cat([x_t_hat, r_t * h, m[:, t, :]], dim=-1)))  # Candidate hidden state\n",
    "        \n",
    "                    # Update hidden state\n",
    "                    h = (1 - z_t) * h + z_t * h_tilde\n",
    "        \n",
    "                # Apply attention weights to the final hidden state\n",
    "                h_weighted = attention_weights * h  # [batch_size, hidden_size]\n",
    "\n",
    "                h_concat = self.dropout(torch.cat([h_weighted, static_embed], dim=-1))  # [B, hidden + static_embedding_size]\n",
    "            \n",
    "                output = torch.sigmoid(self.concat_fc(h_concat)) \n",
    "                return output\n",
    "        \n",
    "        # Training Parameters\n",
    "        input_size = X_temporal_keras.shape[2]  # Temporal input features\n",
    "        hidden_size = 32  # GRU hidden state size\n",
    "        output_size = 1  # Binary classification\n",
    "        static_input_size = X_static_keras.shape[1]  # Static input features\n",
    "        static_embedding_size = 32  # Embedding size for static features\n",
    "        mean_values = torch.tensor(np.nanmean(X_temporal_keras, axis=(0, 1)), dtype=torch.float32)  # Temporal feature means\n",
    "        \n",
    "        \n",
    "        # Convert data to PyTorch tensors\n",
    "        x = torch.tensor(X_temporal_keras, dtype=torch.float32)  # Temporal input\n",
    "        mask = torch.tensor(X_temporal_mask_keras, dtype=torch.float32)  # Masking vector\n",
    "        delta = torch.tensor(X_temporal_delta_keras, dtype=torch.float32)  # Time intervals\n",
    "        static_data = torch.tensor(X_static_keras, dtype=torch.float32)  # Static input\n",
    "        y = torch.tensor(y_keras, dtype=torch.float32)  # Binary labels\n",
    "        \n",
    "        # 7. XGBoost model\n",
    "        def xgboost_model():\n",
    "            return XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "\n",
    "        \n",
    "        def GRUD_Static_AUC(\n",
    "            model_class,\n",
    "            x, mask, delta, static_data, y,\n",
    "            n_folds=5,\n",
    "            n_epochs=200,\n",
    "            learning_rate=0.01,\n",
    "            val_size=0.1,\n",
    "            patience=20\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Perform K-fold cross-validation for GRU-D with static attention,\n",
    "            using an inner validation split for early stopping.\n",
    "            \"\"\"\n",
    "        \n",
    "            kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "            fold_results = []\n",
    "        \n",
    "            for fold, (train_index, test_index) in enumerate(kf.split(x)):\n",
    "                print(f\"Fold {fold + 1}/{n_folds}\")\n",
    "        \n",
    "                # ---- train / test split ----\n",
    "                x_train, x_test = x[train_index], x[test_index]\n",
    "                mask_train, mask_test = mask[train_index], mask[test_index]\n",
    "                delta_train, delta_test = delta[train_index], delta[test_index]\n",
    "                static_train, static_test = static_data[train_index], static_data[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "                threshold = y_train.float().mean().item()\n",
    "        \n",
    "                # ---- train / validation split (inside training fold) ----\n",
    "                (\n",
    "                    x_tr, x_val,\n",
    "                    mask_tr, mask_val,\n",
    "                    delta_tr, delta_val,\n",
    "                    static_tr, static_val,\n",
    "                    y_tr, y_val\n",
    "                ) = train_test_split(\n",
    "                    x_train, mask_train, delta_train, static_train, y_train,\n",
    "                    test_size=val_size,\n",
    "                    stratify=y_train.cpu().numpy(),\n",
    "                    random_state=42\n",
    "                )\n",
    "        \n",
    "                # ---- initialize model ----\n",
    "                model = model_class(\n",
    "                    input_size=x.shape[2],\n",
    "                    hidden_size=hidden_size,\n",
    "                    output_size=output_size,\n",
    "                    mean_values=mean_values,\n",
    "                    static_input_size=static_input_size,\n",
    "                    static_embedding_size=static_embedding_size,\n",
    "                )\n",
    "        \n",
    "                criterion = nn.BCELoss()\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "                # ---- early stopping bookkeeping ----\n",
    "                best_val_loss = np.inf\n",
    "                best_state = None\n",
    "                patience_counter = 0\n",
    "        \n",
    "                # ---- training loop ----\n",
    "                for epoch in range(n_epochs):\n",
    "                    model.train()\n",
    "                    optimizer.zero_grad()\n",
    "        \n",
    "                    outputs = model(x_tr, mask_tr, delta_tr, static_tr)\n",
    "                    loss = criterion(outputs.squeeze(), y_tr.squeeze())\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "        \n",
    "                    # ---- validation ----\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        val_outputs = model(x_val, mask_val, delta_val, static_val)\n",
    "                        val_loss = criterion(val_outputs.squeeze(), y_val.squeeze()).item()\n",
    "        \n",
    "                    if val_loss < best_val_loss - 1e-4:\n",
    "                        best_val_loss = val_loss\n",
    "                        best_state = model.state_dict()\n",
    "                        patience_counter = 0\n",
    "                    else:\n",
    "                        patience_counter += 1\n",
    "        \n",
    "                    if patience_counter >= patience:\n",
    "                        break\n",
    "        \n",
    "                # restore best model\n",
    "                if best_state is not None:\n",
    "                    model.load_state_dict(best_state)\n",
    "        \n",
    "                # ---- test evaluation ----\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(x_test, mask_test, delta_test, static_test).squeeze()\n",
    "                    probabilities = outputs.cpu().numpy()\n",
    "                    predictions = (probabilities > threshold).astype(int)\n",
    "                    y_test_numpy = y_test.int().cpu().numpy()\n",
    "        \n",
    "                    precision = precision_score(y_test_numpy, predictions, zero_division=0)\n",
    "                    recall = recall_score(y_test_numpy, predictions, zero_division=0)\n",
    "                    f1 = f1_score(y_test_numpy, predictions, zero_division=0)\n",
    "        \n",
    "                    roc_auc = roc_auc_score(y_test_numpy, probabilities)\n",
    "        \n",
    "                fold_results.append({\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"f1_score\": f1,\n",
    "                    \"roc_auc\": roc_auc\n",
    "                })\n",
    "        \n",
    "                print(\n",
    "                    f\"Fold {fold + 1} - \"\n",
    "                    f\"Precision: {precision:.4f}, \"\n",
    "                    f\"Recall: {recall:.4f}, \"\n",
    "                    f\"F1: {f1:.4f}, \"\n",
    "                    f\"ROC-AUC: {roc_auc:.4f}\"\n",
    "                )\n",
    "        \n",
    "            # ---- aggregate ----\n",
    "            mean_precision = np.nanmean([r[\"precision\"] for r in fold_results])\n",
    "            mean_recall = np.nanmean([r[\"recall\"] for r in fold_results])\n",
    "            mean_f1 = np.nanmean([r[\"f1_score\"] for r in fold_results])\n",
    "            mean_roc_auc = np.nanmean([r[\"roc_auc\"] for r in fold_results])\n",
    "        \n",
    "            return mean_precision, mean_recall, mean_f1, mean_roc_auc\n",
    "            \n",
    "        # Prepare lists to store the results\n",
    "        models = [\"Logistic Regression\", \"Random Forest\", \"SVM\", \"XGBoost\", \"PredictPTB\", \"RETAIN\", \"GRU-D-Static\"]\n",
    "        \n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f1_scores = []  # List for F1 scores\n",
    "        roc_aucs = []  # List for ROC-AUC scores\n",
    "        \n",
    "        # Function to append results from the evaluate_model function\n",
    "        def append_results(model_name, precision, recall, f1, roc_auc):\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "            roc_aucs.append(roc_auc)\n",
    "        \n",
    "        \n",
    "        # Evaluate Logistic Regression, Random Forest, SVM (2D input)\n",
    "        precision, recall, f1, roc_auc = evaluate_model(logistic_regression, X_sklearn, y_sklearn, model_type=\"sklearn\")\n",
    "        append_results(\"Logistic Regression\", precision, recall, f1, roc_auc)\n",
    "        \n",
    "        precision, recall, f1, roc_auc = evaluate_model(random_forest, X_sklearn, y_sklearn, model_type=\"sklearn\")\n",
    "        append_results(\"Random Forest\", precision, recall, f1, roc_auc)\n",
    "        \n",
    "        precision, recall, f1, roc_auc = evaluate_model(svm_model, X_sklearn, y_sklearn, model_type=\"sklearn\")\n",
    "        append_results(\"SVM\", precision, recall, f1, roc_auc)\n",
    "        \n",
    "        # Evaluate XGBoost (2D input for sklearn-based models)\n",
    "        precision, recall, f1, roc_auc = evaluate_model(xgboost_model, X_sklearn, y_sklearn, model_type=\"sklearn\")\n",
    "        append_results(\"XGBoost\", precision, recall, f1, roc_auc)\n",
    "        \n",
    "        # Evaluate PredictPTB, RETAIN, and GRU (3D input)\n",
    "        precision, recall, f1, roc_auc = evaluate_model(build_predict_ptb, X_keras, y_keras, model_type=\"keras\")\n",
    "        append_results(\"PredictPTB\", precision, recall, f1, roc_auc)\n",
    "        \n",
    "        precision, recall, f1, roc_auc = evaluate_model(build_retain, X_keras, y_keras, model_type=\"keras\")\n",
    "        append_results(\"RETAIN\", precision, recall, f1, roc_auc)\n",
    "        \n",
    "        # Evaluate GRU-D-Static (3D input)\n",
    "        precision, recall, f1, roc_auc = GRUD_Static_AUC(\n",
    "            GRUDWithStaticAttention,\n",
    "            x,\n",
    "            mask,\n",
    "            delta,\n",
    "            static_data,\n",
    "            y,\n",
    "            n_folds=5,\n",
    "            n_epochs=200,\n",
    "            learning_rate=0.01,\n",
    "            val_size=0.1,\n",
    "            patience=20\n",
    "        )\n",
    "        append_results(\"GRU-D-Static\", precision, recall, f1, roc_auc)\n",
    "        \n",
    "        # Create a summary DataFrame\n",
    "        results_summary = pd.DataFrame({\n",
    "            \"Model\": models,\n",
    "            \"Precision\": precisions,\n",
    "            \"Recall\": recalls,\n",
    "            \"F1 Score\": f1_scores,\n",
    "            \"ROC-AUC\": roc_aucs\n",
    "        })\n",
    "        \n",
    "        # Round the numerical values to 3 decimal places\n",
    "        results_summary = results_summary.round(3)\n",
    "\n",
    "        return(results_summary,threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c8fc4ca-2138-44ca-8f61-4831204dc8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\yipeng_wei\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:188: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "Fold 1/5\n",
      "Fold 1 - Precision: 0.1150, Recall: 0.7500, F1: 0.1995, ROC-AUC: 0.9113\n",
      "Fold 2/5\n",
      "Fold 2 - Precision: 0.1118, Recall: 0.6481, F1: 0.1907, ROC-AUC: 0.8211\n",
      "Fold 3/5\n",
      "Fold 3 - Precision: 0.1053, Recall: 0.0444, F1: 0.0625, ROC-AUC: 0.7513\n",
      "Fold 4/5\n",
      "Fold 4 - Precision: 0.1092, Recall: 0.8125, F1: 0.1926, ROC-AUC: 0.9055\n",
      "Fold 5/5\n",
      "Fold 5 - Precision: 0.1082, Recall: 0.7455, F1: 0.1889, ROC-AUC: 0.8655\n",
      "                 Model  Precision  Recall  F1 Score  ROC-AUC\n",
      "0  Logistic Regression      0.076   0.739     0.137    0.845\n",
      "1        Random Forest      0.059   0.822     0.110    0.868\n",
      "2                  SVM      0.071   0.720     0.129    0.840\n",
      "3              XGBoost      0.237   0.641     0.346    0.894\n",
      "4           PredictPTB      0.070   0.690     0.126    0.828\n",
      "5               RETAIN      0.062   0.754     0.115    0.825\n",
      "6         GRU-D-Static      0.110   0.600     0.167    0.851\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "Fold 1/5\n",
      "Fold 1 - Precision: 0.5050, Recall: 0.6338, F1: 0.5621, ROC-AUC: 0.8191\n",
      "Fold 2/5\n",
      "Fold 2 - Precision: 0.5066, Recall: 0.6583, F1: 0.5726, ROC-AUC: 0.8285\n",
      "Fold 3/5\n",
      "Fold 3 - Precision: 0.5253, Recall: 0.6648, F1: 0.5869, ROC-AUC: 0.8343\n",
      "Fold 4/5\n",
      "Fold 4 - Precision: 0.4638, Recall: 0.6992, F1: 0.5577, ROC-AUC: 0.8221\n",
      "Fold 5/5\n",
      "Fold 5 - Precision: 0.5021, Recall: 0.6624, F1: 0.5712, ROC-AUC: 0.8259\n",
      "                 Model  Precision  Recall  F1 Score  ROC-AUC\n",
      "0  Logistic Regression      0.483   0.686     0.567    0.829\n",
      "1        Random Forest      0.435   0.716     0.541    0.817\n",
      "2                  SVM      0.654   0.478     0.552    0.788\n",
      "3              XGBoost      0.468   0.652     0.544    0.811\n",
      "4           PredictPTB      0.445   0.698     0.541    0.818\n",
      "5               RETAIN      0.436   0.725     0.543    0.822\n",
      "6         GRU-D-Static      0.501   0.664     0.570    0.826\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step\n",
      "Fold 1/5\n",
      "Fold 1 - Precision: 0.4366, Recall: 0.6131, F1: 0.5100, ROC-AUC: 0.6670\n",
      "Fold 2/5\n",
      "Fold 2 - Precision: 0.4465, Recall: 0.5649, F1: 0.4987, ROC-AUC: 0.6570\n",
      "Fold 3/5\n",
      "Fold 3 - Precision: 0.4224, Recall: 0.5639, F1: 0.4830, ROC-AUC: 0.6345\n",
      "Fold 4/5\n",
      "Fold 4 - Precision: 0.4335, Recall: 0.6648, F1: 0.5248, ROC-AUC: 0.6696\n",
      "Fold 5/5\n",
      "Fold 5 - Precision: 0.4337, Recall: 0.6014, F1: 0.5040, ROC-AUC: 0.6542\n",
      "                 Model  Precision  Recall  F1 Score  ROC-AUC\n",
      "0  Logistic Regression      0.437   0.593     0.503    0.663\n",
      "1        Random Forest      0.412   0.608     0.491    0.639\n",
      "2                  SVM      0.496   0.334     0.399    0.628\n",
      "3              XGBoost      0.424   0.533     0.472    0.625\n",
      "4           PredictPTB      0.422   0.649     0.511    0.652\n",
      "5               RETAIN      0.430   0.627     0.506    0.660\n",
      "6         GRU-D-Static      0.435   0.602     0.504    0.656\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step\n",
      "Fold 1/5\n",
      "Fold 1 - Precision: 0.0974, Recall: 0.5849, F1: 0.1670, ROC-AUC: 0.6866\n",
      "Fold 2/5\n",
      "Fold 2 - Precision: 0.1068, Recall: 0.5489, F1: 0.1788, ROC-AUC: 0.6814\n",
      "Fold 3/5\n",
      "Fold 3 - Precision: 0.1062, Recall: 0.5815, F1: 0.1795, ROC-AUC: 0.6887\n",
      "Fold 4/5\n",
      "Fold 4 - Precision: 0.1168, Recall: 0.5959, F1: 0.1952, ROC-AUC: 0.6866\n",
      "Fold 5/5\n",
      "Fold 5 - Precision: 0.1296, Recall: 0.5860, F1: 0.2123, ROC-AUC: 0.6733\n",
      "                 Model  Precision  Recall  F1 Score  ROC-AUC\n",
      "0  Logistic Regression      0.112   0.581     0.188    0.683\n",
      "1        Random Forest      0.100   0.620     0.172    0.669\n",
      "2                  SVM      0.089   0.517     0.152    0.617\n",
      "3              XGBoost      0.124   0.407     0.189    0.662\n",
      "4           PredictPTB      0.100   0.632     0.169    0.649\n",
      "5               RETAIN      0.098   0.704     0.170    0.682\n",
      "6         GRU-D-Static      0.111   0.579     0.187    0.683\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step\n",
      "Fold 1/5\n",
      "Fold 1 - Precision: 0.0948, Recall: 0.7021, F1: 0.1671, ROC-AUC: 0.8963\n",
      "Fold 2/5\n",
      "Fold 2 - Precision: 0.1327, Recall: 0.8367, F1: 0.2291, ROC-AUC: 0.9310\n",
      "Fold 3/5\n",
      "Fold 3 - Precision: 0.1053, Recall: 0.8421, F1: 0.1871, ROC-AUC: 0.9455\n",
      "Fold 4/5\n",
      "Fold 4 - Precision: 0.2017, Recall: 0.3429, F1: 0.2540, ROC-AUC: 0.8584\n",
      "Fold 5/5\n",
      "Fold 5 - Precision: 0.1610, Recall: 0.9216, F1: 0.2741, ROC-AUC: 0.9397\n",
      "                 Model  Precision  Recall  F1 Score  ROC-AUC\n",
      "0  Logistic Regression      0.114   0.822     0.200    0.921\n",
      "1        Random Forest      0.085   0.935     0.155    0.937\n",
      "2                  SVM      0.070   0.802     0.129    0.868\n",
      "3              XGBoost      0.261   0.697     0.378    0.944\n",
      "4           PredictPTB      0.089   0.830     0.158    0.926\n",
      "5               RETAIN      0.086   0.890     0.157    0.921\n",
      "6         GRU-D-Static      0.139   0.729     0.222    0.914\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store results\n",
    "results_best = []\n",
    "\n",
    "# Function to process results for an outcome\n",
    "def process_results(outcome):\n",
    "    # Run the function and capture the returned values\n",
    "    results_summary, threshold = print_results(outcome)\n",
    "\n",
    "    print(results_summary)\n",
    "    \n",
    "    # Extract best-performing models for each metric\n",
    "    best_precision = results_summary.loc[results_summary[\"Precision\"].idxmax(), [\"Model\", \"Precision\"]]\n",
    "    best_recall = results_summary.loc[results_summary[\"Recall\"].idxmax(), [\"Model\", \"Recall\"]]\n",
    "    best_f1 = results_summary.loc[results_summary[\"F1 Score\"].idxmax(), [\"Model\", \"F1 Score\"]]\n",
    "    best_roc_auc = results_summary.loc[results_summary[\"ROC-AUC\"].idxmax(), [\"Model\", \"ROC-AUC\"]]\n",
    "\n",
    "    # Append results to the list\n",
    "    results_best.append({\n",
    "        \"Outcome\": outcome,\n",
    "        \"Prevalence (%)\": f\"{threshold * 100:.2f}\",\n",
    "        \"Best Precision Model\": best_precision[\"Model\"],\n",
    "        \"Best Precision Score\": f\"{best_precision['Precision']:.3f}\",\n",
    "        \"Best Recall Model\": best_recall[\"Model\"],\n",
    "        \"Best Recall Score\": f\"{best_recall['Recall']:.3f}\",\n",
    "        \"Best F1 Score Model\": best_f1[\"Model\"],\n",
    "        \"Best F1 Score\": f\"{best_f1['F1 Score']:.3f}\",\n",
    "        \"Best ROC-AUC Model\": best_roc_auc[\"Model\"],\n",
    "        \"Best ROC-AUC Score\": f\"{best_roc_auc['ROC-AUC']:.3f}\"\n",
    "    })\n",
    "\n",
    "# List of outcomes to process\n",
    "outcomes = [\"STILLBIRTH_SIGNS_LIFE\",\"LBW2500_ANY\", \"SVN\", \"NEARMISS\", \"NEO_DTH\"]\n",
    "\n",
    "# Run the function for each outcome\n",
    "for outcome in outcomes:\n",
    "    process_results(outcome)\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "summary_df = pd.DataFrame(results_best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
