{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c308c8f0-5023-429a-b993-f0fd0235d7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/10\n",
      "Fold 1 - Precision: 0.9930, Recall: 0.8301, F1 Score: 0.9043, ROC-AUC: 0.8589\n",
      "Fold 2/10\n",
      "Fold 2 - Precision: 0.9886, Recall: 0.8441, F1 Score: 0.9106, ROC-AUC: 0.8260\n",
      "Fold 3/10\n",
      "Fold 3 - Precision: 0.9881, Recall: 0.8137, F1 Score: 0.8925, ROC-AUC: 0.7882\n",
      "Fold 4/10\n",
      "Fold 4 - Precision: 0.9907, Recall: 0.8392, F1 Score: 0.9087, ROC-AUC: 0.8164\n",
      "Fold 5/10\n",
      "Fold 5 - Precision: 0.9929, Recall: 0.8258, F1 Score: 0.9017, ROC-AUC: 0.8685\n",
      "Fold 6/10\n",
      "Fold 6 - Precision: 0.9903, Recall: 0.8008, F1 Score: 0.8855, ROC-AUC: 0.8324\n",
      "Fold 7/10\n",
      "Fold 7 - Precision: 0.9865, Recall: 0.8622, F1 Score: 0.9202, ROC-AUC: 0.8514\n",
      "Fold 8/10\n",
      "Fold 8 - Precision: 0.9971, Recall: 0.6797, F1 Score: 0.8084, ROC-AUC: 0.9135\n",
      "Fold 9/10\n",
      "Fold 9 - Precision: 0.9930, Recall: 0.8233, F1 Score: 0.9002, ROC-AUC: 0.8747\n",
      "Fold 10/10\n",
      "Fold 10 - Precision: 0.9913, Recall: 0.8850, F1 Score: 0.9351, ROC-AUC: 0.8475\n",
      "Mean Precision: 0.9912\n",
      "Mean Recall: 0.8204\n",
      "Mean F1 Score: 0.8967\n",
      "Mean ROC-AUC: 0.8478\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data from a CSV file\n",
    "df = pd.read_csv('D:/Users/yipeng_wei/Documents/dl data/2024-06-28/df_dl_continous.csv')\n",
    "df_mask = pd.read_csv('D:/Users/yipeng_wei/Documents/dl data/2024-06-28/df_dl_continous_mask.csv')\n",
    "df_delta = pd.read_csv('D:/Users/yipeng_wei/Documents/dl data/2024-06-28/df_dl_continous_delta.csv')\n",
    "\n",
    "# Drop rows with NaN in the target column before any operations\n",
    "df = df.dropna(subset=['STILLBIRTH_SIGNS_LIFE'])\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df_mask = df_mask.dropna(subset=['STILLBIRTH_SIGNS_LIFE'])\n",
    "df_mask = df_mask.reset_index(drop=True)\n",
    "\n",
    "df_delta = df_delta.dropna(subset=['STILLBIRTH_SIGNS_LIFE'])\n",
    "df_delta = df_delta.reset_index(drop=True)\n",
    "\n",
    "# Selecting features and the target\n",
    "features_categorical = [\"TYPE_VISIT\",\n",
    "            \"INF_PRES_CEPH\",\"INF_PRES_BREECH\",\"INF_PRES_TRANS\",\"INF_PRES_BROW\",\"INF_PRES_OTHER\",\n",
    "            \"ASPHYXIA_IND\",\"STILLBIRTH_IND\",\"PRETERM_IND\",\"POSTTERM_IND\",\"GEST_HTN_IND\",\"PREECLAMPSIA_IND\",\"GEST_DIAB_IND\",\"PREMATURE_RUPTURE_IND\",\"OBSTR_LABOR_IND\",\n",
    "            \"miscarriage\",\"paid_work\",\"PARITY_2\",\"PARITY_1\",\"WEALTH_QUINT_1\",\"WEALTH_QUINT_2\",\"WEALTH_QUINT_3\",\"WEALTH_QUINT_4\",\"SCHOOL_MORE10\",\"water_improved\",\"toilet_improved\",\"M03_STOVE_FCORRESR_ind\",\"hh_smoke\",\"AGE_GROUP\",\"bmi_index\",\n",
    "            \"MEM_CES\",\"MEM_ART\",\"MEM_SPON\",\"LABOR_ANY\",\"PRO_LABOR\",\"OBS_LABOR\",\"PRETERM_ANY\",\n",
    "            \"HEM_APH\",\"HIV_POSITIVE_ENROLL\",\"SYPH_POSITIVE_ENROLL\",\"GON_POSITIVE_ENROLL\",\"CHL_POSITIVE_ENROLL\",\"GENU_POSITIVE_ENROLL\",\"OTHR_POSITIVE_ENROLL\",\"MAL_POSITIVE_ENROLL\",\"HBV_POSITIVE_ENROLL\",\"HCV_POSITIVE_ENROLL\",\"TB_SYMP_POSITIVE_ENROLL\",\n",
    "            \"M04_IRON_Supplement\",\"M04_IFA_CMOCCUR\",\"M04_CALCIUM_CMOCCUR\",\"M04_VITAMIN_A_CMOCCUR\",\"M04_MICRONUTRIENT_CMOCCUR\",\"M04_ANTHELMINTHIC_CMOCCUR\"]\n",
    "\n",
    "features_continous = [\"age\",\"bmi_enroll\",\"muac\",\"M08_CBC_MCV_LBORRES\", \"M08_VITB12_COB_LBORRES\", \"M08_VITB12_HOL_LBORRES\", \"M08_FOLATE_PLASMA_NMOLL_LBORRES\", \"M08_IRON_TOT_UGDL_LBORRES\",\n",
    "                \"M08_VITA_UGDL_LBORRES\", \"M08_FERRITIN_LBORRES\",\"M08_IODINE_LBORRES\",\"M08_RBP4_LBORRES\",\"M08_CRP_LBORRES\",\"M08_AGP_LBORRES\", \"M08_CBC_HB_LBORRES\",\"M08_HBA1C_LBORRES\"]\n",
    "\n",
    "# Separate categorical and continuous features\n",
    "df_categorical = df[features_categorical]\n",
    "df_continuous = df[features_continous]\n",
    "\n",
    "# Rescale continuous features to (0, 1) range\n",
    "scaler = MinMaxScaler()\n",
    "df_continuous_scaled = pd.DataFrame(scaler.fit_transform(df_continuous), columns=features_continous)\n",
    "\n",
    "# Concatenate scaled continuous features with categorical features\n",
    "X = pd.concat([df_categorical, df_continuous_scaled], axis=1)\n",
    "\n",
    "features_static = [\"INF_PRES_CEPH\",\"INF_PRES_BREECH\",\"INF_PRES_TRANS\",\"INF_PRES_BROW\",\"INF_PRES_OTHER\",\n",
    "                \"ASPHYXIA_IND\",\"STILLBIRTH_IND\",\"PRETERM_IND\",\"POSTTERM_IND\",\"GEST_HTN_IND\",\"PREECLAMPSIA_IND\",\"GEST_DIAB_IND\",\"PREMATURE_RUPTURE_IND\",\"OBSTR_LABOR_IND\",\n",
    "                \"miscarriage\",\"paid_work\",\"PARITY_2\",\"PARITY_1\",\"WEALTH_QUINT_1\",\"WEALTH_QUINT_2\",\"WEALTH_QUINT_3\",\"WEALTH_QUINT_4\",\"SCHOOL_MORE10\",\"water_improved\",\"toilet_improved\",\"M03_STOVE_FCORRESR_ind\",\"hh_smoke\",\"AGE_GROUP\",\"bmi_index\",\n",
    "                \"age\",\"bmi_enroll\",\"muac\",\n",
    "                \"MEM_CES\",\"MEM_ART\",\"MEM_SPON\",\"LABOR_ANY\",\"PRO_LABOR\",\"OBS_LABOR\",\"PRETERM_ANY\",\n",
    "                \"HEM_APH\",\"HIV_POSITIVE_ENROLL\",\"SYPH_POSITIVE_ENROLL\",\"GON_POSITIVE_ENROLL\",\"CHL_POSITIVE_ENROLL\",\"GENU_POSITIVE_ENROLL\",\"OTHR_POSITIVE_ENROLL\",\"MAL_POSITIVE_ENROLL\",\"HBV_POSITIVE_ENROLL\",\"HCV_POSITIVE_ENROLL\",\"TB_SYMP_POSITIVE_ENROLL\"]\n",
    "\n",
    "features_temporal = [\"TYPE_VISIT\",\"M04_IRON_Supplement\",\"M04_IFA_CMOCCUR\",\"M04_CALCIUM_CMOCCUR\",\"M04_VITAMIN_A_CMOCCUR\",\"M04_MICRONUTRIENT_CMOCCUR\",\"M04_ANTHELMINTHIC_CMOCCUR\",\n",
    "                     \"M08_CBC_MCV_LBORRES\", \"M08_VITB12_COB_LBORRES\", \"M08_VITB12_HOL_LBORRES\", \"M08_FOLATE_PLASMA_NMOLL_LBORRES\", \"M08_IRON_TOT_UGDL_LBORRES\",\n",
    "                \"M08_VITA_UGDL_LBORRES\", \"M08_FERRITIN_LBORRES\",\"M08_IODINE_LBORRES\",\"M08_RBP4_LBORRES\",\"M08_CRP_LBORRES\",\"M08_AGP_LBORRES\", \"M08_CBC_HB_LBORRES\",\"M08_HBA1C_LBORRES\"]\n",
    "\n",
    "##Static and temporal data\n",
    "X_static = X[features_static]\n",
    "\n",
    "X_temporal = X[features_temporal]\n",
    "X_temporal_delta = df_delta[features_temporal]\n",
    "X_temporal_mask = df_mask[features_temporal]\n",
    "\n",
    "y = df['STILLBIRTH_SIGNS_LIFE']\n",
    "\n",
    "# One-hot encoding for non-binary categorical features in X\n",
    "X = pd.get_dummies(X, columns=['TYPE_VISIT'], dummy_na=False)\n",
    "X_temporal = pd.get_dummies(X_temporal, columns=['TYPE_VISIT'], dummy_na=False)\n",
    "X_temporal_delta = pd.get_dummies(X_temporal_delta, columns=['TYPE_VISIT'], dummy_na=False)\n",
    "X_temporal_mask = pd.get_dummies(X_temporal_mask, columns=['TYPE_VISIT'], dummy_na=False)\n",
    "\n",
    "# Preprocess for Keras models (3D matrix)\n",
    "n_samples = X_temporal.shape[0] // 5 \n",
    "X_static_keras = X_static.iloc[::5].values.astype('float32')\n",
    "\n",
    "X_temporal_keras = X_temporal.values.reshape(n_samples, 5, X_temporal.shape[1]).astype('float32')\n",
    "X_temporal_delta_keras = X_temporal_delta.values.reshape(n_samples, 5, X_temporal_delta.shape[1]).astype('float32')\n",
    "X_temporal_mask_keras = X_temporal_mask.values.reshape(n_samples, 5, X_temporal_mask.shape[1]).astype('float32')\n",
    "\n",
    "X_keras = X.values.reshape(n_samples, 5, X.shape[1]).astype('float32')\n",
    "\n",
    "# Align y for Keras models\n",
    "y_keras = y.iloc[::5].values.astype('float32')\n",
    "\n",
    "#### 7. GRUD With Static Attention\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "class GRUDWithStaticAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, mean_values, static_input_size, static_embedding_size):\n",
    "        \"\"\"\n",
    "        GRU-D model with attention mechanism generated from static data.\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): Number of input features for temporal data.\n",
    "            hidden_size (int): Number of hidden units in GRU.\n",
    "            output_size (int): Number of output classes (for classification tasks).\n",
    "            mean_values (torch.Tensor): Empirical mean values for each input feature.\n",
    "            static_input_size (int): Number of input features for static data.\n",
    "            static_embedding_size (int): Size of the embedding for static data.\n",
    "        \"\"\"\n",
    "        super(GRUDWithStaticAttention, self).__init__()\n",
    "        \n",
    "        # Temporal (GRU-D) parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.mean_values = mean_values\n",
    "        \n",
    "        self.gamma_x = nn.Linear(input_size, input_size)\n",
    "        \n",
    "        # GRU gate parameters (temporal part) with combined input, hidden state, and mask\n",
    "        self.zl = nn.Linear(input_size + hidden_size + input_size, hidden_size)  # Update gate\n",
    "        self.rl = nn.Linear(input_size + hidden_size + input_size, hidden_size)  # Reset gate\n",
    "        self.hl = nn.Linear(input_size + hidden_size + input_size, hidden_size)  # Candidate hidden state\n",
    "\n",
    "        # Embedding or processing static data\n",
    "        self.static_fc = nn.Linear(static_input_size, static_embedding_size)\n",
    "\n",
    "        # Attention mechanism based on static data\n",
    "        self.attention = nn.Linear(static_embedding_size, hidden_size)\n",
    "\n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, m, delta, static_data):\n",
    "        \"\"\"\n",
    "        Forward pass of GRU-D with static data attention.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Temporal input data [batch_size, seq_len, input_size].\n",
    "            m (torch.Tensor): Masking vector [batch_size, seq_len, input_size].\n",
    "            delta (torch.Tensor): Time intervals [batch_size, seq_len, input_size].\n",
    "            static_data (torch.Tensor): Static data input [batch_size, static_input_size].\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Process static data to generate attention weights\n",
    "        static_embed = torch.relu(self.static_fc(static_data))  # [batch_size, static_embedding_size]\n",
    "        \n",
    "        # Generate attention weights from static data\n",
    "        attention_weights = torch.sigmoid(self.attention(static_embed))  # [batch_size, hidden_size]\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        h = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # Corrected input decay mechanism\n",
    "            gamma_x_t = torch.exp(-F.relu(self.gamma_x(delta[:, t, :])))\n",
    "            x_t_hat = m[:, t, :] * x[:, t, :] + (1 - m[:, t, :]) * (gamma_x_t * self.mean_values + (1 - gamma_x_t) * x[:, t, :])\n",
    "            \n",
    "            # Concatenate input, hidden state, and mask\n",
    "            combined = torch.cat([x_t_hat, h, m[:, t, :]], dim=-1)\n",
    "\n",
    "            # GRU gates\n",
    "            r_t = torch.sigmoid(self.rl(combined))  # Reset gate\n",
    "            z_t = torch.sigmoid(self.zl(combined))  # Update gate\n",
    "            h_tilde = torch.tanh(self.hl(torch.cat([x_t_hat, r_t * h, m[:, t, :]], dim=-1)))  # Candidate hidden state\n",
    "\n",
    "            # Update hidden state\n",
    "            h = (1 - z_t) * h + z_t * h_tilde\n",
    "\n",
    "        # Apply attention weights to the final hidden state\n",
    "        h_weighted = attention_weights * h  # [batch_size, hidden_size]\n",
    "\n",
    "        # Final output\n",
    "        output = self.fc(h_weighted)\n",
    "\n",
    "        # Sigmoid for binary classification\n",
    "        output = torch.sigmoid(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Training Parameters\n",
    "input_size = X_temporal_keras.shape[2]  # Temporal input features\n",
    "hidden_size = 64  # GRU hidden state size\n",
    "output_size = 1  # Binary classification\n",
    "static_input_size = X_static_keras.shape[1]  # Static input features\n",
    "static_embedding_size = 16  # Embedding size for static features\n",
    "mean_values = torch.tensor(np.nanmean(X_temporal_keras, axis=(0, 1)), dtype=torch.float32)  # Temporal feature means\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x = torch.tensor(X_temporal_keras, dtype=torch.float32)  # Temporal input\n",
    "mask = torch.tensor(X_temporal_mask_keras, dtype=torch.float32)  # Masking vector\n",
    "delta = torch.tensor(X_temporal_delta_keras, dtype=torch.float32)  # Time intervals\n",
    "static_data = torch.tensor(X_static_keras, dtype=torch.float32)  # Static input\n",
    "y = torch.tensor(y_keras, dtype=torch.float32)  # Binary labels\n",
    "\n",
    "# Initialize the model\n",
    "model = GRUDWithStaticAttention(input_size, hidden_size, output_size, mean_values, static_input_size, static_embedding_size)\n",
    "\n",
    "def cross_validate_model_with_auc(model_class, x, mask, delta, static_data, y, n_folds=10, n_epochs=100, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Perform 10-fold cross-validation for the GRUDWithStaticAttention model with ROC-AUC.\n",
    "    \n",
    "    Args:\n",
    "        model_class: The class of the model to instantiate.\n",
    "        x (torch.Tensor): Temporal input data [batch_size, seq_len, input_size].\n",
    "        mask (torch.Tensor): Masking vector [batch_size, seq_len, input_size].\n",
    "        delta (torch.Tensor): Time intervals [batch_size, seq_len, input_size].\n",
    "        static_data (torch.Tensor): Static input data [batch_size, static_input_size].\n",
    "        y (torch.Tensor): Binary labels for classification [batch_size, 1].\n",
    "        n_folds (int): Number of folds for cross-validation.\n",
    "        n_epochs (int): Number of epochs for training.\n",
    "        learning_rate (float): Learning rate for optimizer.\n",
    "    \n",
    "    Returns:\n",
    "        mean_precision, mean_recall, mean_f1, mean_roc_auc: Mean metrics across folds.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(x)):\n",
    "        print(f\"Fold {fold + 1}/{n_folds}\")\n",
    "\n",
    "        # Split data into train and test sets\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        mask_train, mask_test = mask[train_index], mask[test_index]\n",
    "        delta_train, delta_test = delta[train_index], delta[test_index]\n",
    "        static_train, static_test = static_data[train_index], static_data[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Initialize the model\n",
    "        model = model_class(\n",
    "            input_size=x.shape[2],\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=output_size,\n",
    "            mean_values=mean_values,\n",
    "            static_input_size=static_input_size,\n",
    "            static_embedding_size=static_embedding_size,\n",
    "        )\n",
    "\n",
    "        # Binary Cross-Entropy Loss and Adam Optimizer\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(n_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(x_train, mask_train, delta_train, static_train)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs.squeeze(), y_train.squeeze())\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate the model on test set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(x_test, mask_test, delta_test, static_test).squeeze()\n",
    "            probabilities = outputs.cpu().numpy()\n",
    "            predictions = (probabilities > 0.97).astype(int)\n",
    "            y_test_numpy = y_test.int().cpu().numpy()\n",
    "\n",
    "            # Compute evaluation metrics\n",
    "            precision = precision_score(y_test_numpy, predictions, zero_division=0)\n",
    "            recall = recall_score(y_test_numpy, predictions, zero_division=0)\n",
    "            f1 = f1_score(y_test_numpy, predictions, zero_division=0)\n",
    "            roc_auc = roc_auc_score(y_test_numpy, probabilities)\n",
    "\n",
    "            fold_results.append({\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1_score\": f1,\n",
    "                \"roc_auc\": roc_auc\n",
    "            })\n",
    "\n",
    "            print(f\"Fold {fold + 1} - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    # Compute mean metrics across all folds\n",
    "    mean_precision = np.mean([result['precision'] for result in fold_results])\n",
    "    mean_recall = np.mean([result['recall'] for result in fold_results])\n",
    "    mean_f1 = np.mean([result['f1_score'] for result in fold_results])\n",
    "    mean_roc_auc = np.mean([result['roc_auc'] for result in fold_results])\n",
    "\n",
    "    return mean_precision, mean_recall, mean_f1, mean_roc_auc\n",
    "\n",
    "# Evaluate GRU-D-Static (3D input)\n",
    "precision, recall, f1, roc_auc = cross_validate_model_with_auc(\n",
    "    GRUDWithStaticAttention,\n",
    "    x,\n",
    "    mask,\n",
    "    delta,\n",
    "    static_data,\n",
    "    y,\n",
    "    n_folds=10,\n",
    "    n_epochs=100,\n",
    "    learning_rate=0.015\n",
    ")\n",
    "\n",
    "print(f\"Mean Precision: {precision:.4f}\")\n",
    "print(f\"Mean Recall: {recall:.4f}\")\n",
    "print(f\"Mean F1 Score: {f1:.4f}\")\n",
    "print(f\"Mean ROC-AUC: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcede453-8d6b-47b4-90e1-796bb828b7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
