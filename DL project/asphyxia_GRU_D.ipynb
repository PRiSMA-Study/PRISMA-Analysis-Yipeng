{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a5904b6-37e6-4b5c-a297-ec8027e65a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data from a CSV file\n",
    "df = pd.read_csv('D:/Users/yipeng_wei/Documents/dl data/2024-06-28/df_dl_continous.csv')\n",
    "df_mask = pd.read_csv('D:/Users/yipeng_wei/Documents/dl data/2024-06-28/df_dl_continous_mask.csv')\n",
    "df_delta = pd.read_csv('D:/Users/yipeng_wei/Documents/dl data/2024-06-28/df_dl_continous_delta.csv')\n",
    "\n",
    "# Drop rows with NaN in the target column before any operations\n",
    "df = df.dropna(subset=['INF_ASPH'])\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df_mask = df_mask.dropna(subset=['INF_ASPH'])\n",
    "df_mask = df_mask.reset_index(drop=True)\n",
    "\n",
    "df_delta = df_delta.dropna(subset=['INF_ASPH'])\n",
    "df_delta = df_delta.reset_index(drop=True)\n",
    "\n",
    "# Selecting features and the target\n",
    "features_categorical = [\"TYPE_VISIT\",\n",
    "            \"INF_PRES_CEPH\",\"INF_PRES_BREECH\",\"INF_PRES_TRANS\",\"INF_PRES_BROW\",\"INF_PRES_OTHER\",\n",
    "            \"ASPHYXIA_IND\",\"STILLBIRTH_IND\",\"PRETERM_IND\",\"POSTTERM_IND\",\"GEST_HTN_IND\",\"PREECLAMPSIA_IND\",\"GEST_DIAB_IND\",\"PREMATURE_RUPTURE_IND\",\"OBSTR_LABOR_IND\",\n",
    "            \"miscarriage\",\"paid_work\",\"PARITY_2\",\"PARITY_1\",\"WEALTH_QUINT_1\",\"WEALTH_QUINT_2\",\"WEALTH_QUINT_3\",\"WEALTH_QUINT_4\",\"SCHOOL_MORE10\",\"water_improved\",\"toilet_improved\",\"M03_STOVE_FCORRESR_ind\",\"hh_smoke\",\"AGE_GROUP\",\"bmi_index\",\n",
    "            \"MEM_CES\",\"MEM_ART\",\"MEM_SPON\",\"LABOR_ANY\",\"PRO_LABOR\",\"OBS_LABOR\",\"PRETERM_ANY\",\n",
    "            \"HEM_APH\",\"HIV_POSITIVE_ENROLL\",\"SYPH_POSITIVE_ENROLL\",\"GON_POSITIVE_ENROLL\",\"CHL_POSITIVE_ENROLL\",\"GENU_POSITIVE_ENROLL\",\"OTHR_POSITIVE_ENROLL\",\"MAL_POSITIVE_ENROLL\",\"HBV_POSITIVE_ENROLL\",\"HCV_POSITIVE_ENROLL\",\"TB_SYMP_POSITIVE_ENROLL\",\n",
    "            \"M04_IRON_Supplement\",\"M04_IFA_CMOCCUR\",\"M04_CALCIUM_CMOCCUR\",\"M04_VITAMIN_A_CMOCCUR\",\"M04_MICRONUTRIENT_CMOCCUR\",\"M04_ANTHELMINTHIC_CMOCCUR\"]\n",
    "\n",
    "features_continous = [\"age\",\"bmi_enroll\",\"muac\",\"M08_CBC_MCV_LBORRES\", \"M08_VITB12_COB_LBORRES\", \"M08_VITB12_HOL_LBORRES\", \"M08_FOLATE_PLASMA_NMOLL_LBORRES\", \"M08_IRON_TOT_UGDL_LBORRES\",\n",
    "                \"M08_VITA_UGDL_LBORRES\", \"M08_FERRITIN_LBORRES\",\"M08_IODINE_LBORRES\",\"M08_RBP4_LBORRES\",\"M08_CRP_LBORRES\",\"M08_AGP_LBORRES\", \"M08_CBC_HB_LBORRES\",\"M08_HBA1C_LBORRES\"]\n",
    "\n",
    "# Separate categorical and continuous features\n",
    "df_categorical = df[features_categorical]\n",
    "df_continuous = df[features_continous]\n",
    "\n",
    "# Rescale continuous features to (0, 1) range\n",
    "scaler = MinMaxScaler()\n",
    "df_continuous_scaled = pd.DataFrame(scaler.fit_transform(df_continuous), columns=features_continous)\n",
    "\n",
    "# Concatenate scaled continuous features with categorical features\n",
    "X = pd.concat([df_categorical, df_continuous_scaled], axis=1)\n",
    "\n",
    "features_static = [\"INF_PRES_CEPH\",\"INF_PRES_BREECH\",\"INF_PRES_TRANS\",\"INF_PRES_BROW\",\"INF_PRES_OTHER\",\n",
    "                \"ASPHYXIA_IND\",\"STILLBIRTH_IND\",\"PRETERM_IND\",\"POSTTERM_IND\",\"GEST_HTN_IND\",\"PREECLAMPSIA_IND\",\"GEST_DIAB_IND\",\"PREMATURE_RUPTURE_IND\",\"OBSTR_LABOR_IND\",\n",
    "                \"miscarriage\",\"paid_work\",\"PARITY_2\",\"PARITY_1\",\"WEALTH_QUINT_1\",\"WEALTH_QUINT_2\",\"WEALTH_QUINT_3\",\"WEALTH_QUINT_4\",\"SCHOOL_MORE10\",\"water_improved\",\"toilet_improved\",\"M03_STOVE_FCORRESR_ind\",\"hh_smoke\",\"AGE_GROUP\",\"bmi_index\",\n",
    "                \"age\",\"bmi_enroll\",\"muac\",\n",
    "                \"MEM_CES\",\"MEM_ART\",\"MEM_SPON\",\"LABOR_ANY\",\"PRO_LABOR\",\"OBS_LABOR\",\"PRETERM_ANY\",\n",
    "                \"HEM_APH\",\"HIV_POSITIVE_ENROLL\",\"SYPH_POSITIVE_ENROLL\",\"GON_POSITIVE_ENROLL\",\"CHL_POSITIVE_ENROLL\",\"GENU_POSITIVE_ENROLL\",\"OTHR_POSITIVE_ENROLL\",\"MAL_POSITIVE_ENROLL\",\"HBV_POSITIVE_ENROLL\",\"HCV_POSITIVE_ENROLL\",\"TB_SYMP_POSITIVE_ENROLL\"]\n",
    "\n",
    "features_temporal = [\"TYPE_VISIT\",\"M04_IRON_Supplement\",\"M04_IFA_CMOCCUR\",\"M04_CALCIUM_CMOCCUR\",\"M04_VITAMIN_A_CMOCCUR\",\"M04_MICRONUTRIENT_CMOCCUR\",\"M04_ANTHELMINTHIC_CMOCCUR\",\n",
    "                     \"M08_CBC_MCV_LBORRES\", \"M08_VITB12_COB_LBORRES\", \"M08_VITB12_HOL_LBORRES\", \"M08_FOLATE_PLASMA_NMOLL_LBORRES\", \"M08_IRON_TOT_UGDL_LBORRES\",\n",
    "                \"M08_VITA_UGDL_LBORRES\", \"M08_FERRITIN_LBORRES\",\"M08_IODINE_LBORRES\",\"M08_RBP4_LBORRES\",\"M08_CRP_LBORRES\",\"M08_AGP_LBORRES\", \"M08_CBC_HB_LBORRES\",\"M08_HBA1C_LBORRES\"]\n",
    "\n",
    "##Static and temporal data\n",
    "X_static = X[features_static]\n",
    "\n",
    "X_temporal = X[features_temporal]\n",
    "X_temporal_delta = df_delta[features_temporal]\n",
    "X_temporal_mask = df_mask[features_temporal]\n",
    "\n",
    "y = df['INF_ASPH']\n",
    "\n",
    "# One-hot encoding for non-binary categorical features in X\n",
    "X = pd.get_dummies(X, columns=['TYPE_VISIT'], dummy_na=False)\n",
    "X_temporal = pd.get_dummies(X_temporal, columns=['TYPE_VISIT'], dummy_na=False)\n",
    "X_temporal_delta = pd.get_dummies(X_temporal_delta, columns=['TYPE_VISIT'], dummy_na=False)\n",
    "X_temporal_mask = pd.get_dummies(X_temporal_mask, columns=['TYPE_VISIT'], dummy_na=False)\n",
    "\n",
    "# Preprocess for Keras models (3D matrix)\n",
    "n_samples = X_temporal.shape[0] // 5 \n",
    "X_static_keras = X_static.iloc[::5].values.astype('float32')\n",
    "\n",
    "X_temporal_keras = X_temporal.values.reshape(n_samples, 5, X_temporal.shape[1]).astype('float32')\n",
    "X_temporal_delta_keras = X_temporal_delta.values.reshape(n_samples, 5, X_temporal_delta.shape[1]).astype('float32')\n",
    "X_temporal_mask_keras = X_temporal_mask.values.reshape(n_samples, 5, X_temporal_mask.shape[1]).astype('float32')\n",
    "\n",
    "X_keras = X.values.reshape(n_samples, 5, X.shape[1]).astype('float32')\n",
    "\n",
    "# Align y for Keras models\n",
    "y_keras = y.iloc[::5].values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ca21e58-bb37-4112-a266-00fa6be0101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, GRU, Input, Multiply, TimeDistributed, Softmax, Lambda, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def evaluate_model(model_func, X, y, model_type=\"sklearn\", threshold=0.906):\n",
    "    # 10-fold cross-validation\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Lists to store scores for each fold\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []  # List to store F1 scores\n",
    "    roc_auc_scores = []  # List to store ROC-AUC scores\n",
    "    \n",
    "    # Cross-validation loop\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        if model_type == \"sklearn\":\n",
    "            # Split training data into 90% training and 10% validation\n",
    "            train_size = int(0.9 * len(X_train))\n",
    "            X_train_part, X_val = X_train[:train_size], X_train[train_size:]\n",
    "            y_train_part, y_val = y_train[:train_size], y_train[train_size:]\n",
    "            \n",
    "            # scikit-learn model: Logistic Regression, Random Forest, SVM\n",
    "            model = model_func()\n",
    "            model.fit(X_train_part, y_train_part)  # Train on 90% of the training data\n",
    "\n",
    "            y_val_pred_prob = model.predict_proba(X_val)[:, 1]  # Predicted probabilities for positive class\n",
    "            y_pred_prob = model.predict_proba(X_test)[:, 1]  # Predicted probabilities for positive class:\n",
    "        \n",
    "        else:  # Keras models: GRU, RETAIN, PredictPTB\n",
    "            X_train_part, X_val = X_train[:int(0.9 * len(X_train))], X_train[int(0.9 * len(X_train)):]\n",
    "            y_train_part, y_val = y_train[:int(0.9 * len(y_train))], y_train[int(0.9 * len(y_train)):]\n",
    "            \n",
    "            model = model_func(input_shape=(X_train_part.shape[1], X_train_part.shape[2]))\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "            \n",
    "            # Train the Keras model with early stopping\n",
    "            model.fit(X_train_part, y_train_part, epochs=200, batch_size=16, verbose=0, \n",
    "                      validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "            \n",
    "            y_pred_prob = model.predict(X_test)  # Predicted probabilities\n",
    "        \n",
    "        # Convert predicted probabilities to binary predictions (based on threshold 0.906)\n",
    "        y_pred = (y_pred_prob > threshold).astype('int32')\n",
    "        \n",
    "        # Calculate metrics for this fold\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)  # Calculate F1 score\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_prob)  # Calculate ROC-AUC score\n",
    "        \n",
    "        # Append scores\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)  # Append F1 score\n",
    "        roc_auc_scores.append(roc_auc)  # Append ROC-AUC score\n",
    "    \n",
    "    # Compute and return mean scores across all folds\n",
    "    mean_precision = np.mean(precision_scores)\n",
    "    mean_recall = np.mean(recall_scores)\n",
    "    mean_f1 = np.mean(f1_scores)  # Mean F1 score\n",
    "    mean_roc_auc = np.mean(roc_auc_scores)  # Mean ROC-AUC score\n",
    "    \n",
    "    return mean_precision, mean_recall, mean_f1, mean_roc_auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b1b27c3-3683-4cd9-a076-d3a050060d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. PredictPTB model\n",
    "def build_predict_ptb(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    visit_embedding = TimeDistributed(Dense(64, activation='relu'))(inputs)\n",
    "    rnn_beta, _ = GRU(32, return_sequences=True, return_state=True)(visit_embedding)\n",
    "    beta = TimeDistributed(Dense(64, activation='tanh'))(rnn_beta)\n",
    "    elementwise_prod = Multiply()([beta, visit_embedding])\n",
    "    context_vector = Lambda(lambda x: tf.reduce_sum(x, axis=1))(elementwise_prod)\n",
    "    output = Dense(1, activation='sigmoid')(context_vector)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "# 5. RETAIN model\n",
    "def build_retain(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    visit_embedding = TimeDistributed(Dense(64, activation='relu'))(inputs)\n",
    "    rnn_alpha, _ = GRU(32, return_sequences=True, return_state=True)(visit_embedding)\n",
    "    attention_scores = TimeDistributed(Dense(1))(rnn_alpha)\n",
    "    alpha = Softmax(axis=1)(attention_scores)\n",
    "    rnn_beta, _ = GRU(32, return_sequences=True, return_state=True)(visit_embedding)\n",
    "    beta = TimeDistributed(Dense(64, activation='tanh'))(rnn_beta)\n",
    "    weighted_context = Multiply()([alpha, Multiply()([beta, visit_embedding])])\n",
    "    context_vector = Lambda(lambda x: tf.reduce_sum(x, axis=1))(weighted_context)\n",
    "    output = Dense(1, activation='sigmoid')(context_vector)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "# 6. GRU model\n",
    "def build_gru(input_shape):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        GRU(32, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0cf5ffb-80d1-44aa-bc17-326670f61547",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 7. GRUD With Static Attention\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "class GRUDWithStaticAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, mean_values, static_input_size, static_embedding_size):\n",
    "        \"\"\"\n",
    "        GRU-D model with attention mechanism generated from static data.\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): Number of input features for temporal data.\n",
    "            hidden_size (int): Number of hidden units in GRU.\n",
    "            output_size (int): Number of output classes (for classification tasks).\n",
    "            mean_values (torch.Tensor): Empirical mean values for each input feature.\n",
    "            static_input_size (int): Number of input features for static data.\n",
    "            static_embedding_size (int): Size of the embedding for static data.\n",
    "        \"\"\"\n",
    "        super(GRUDWithStaticAttention, self).__init__()\n",
    "        \n",
    "        # Temporal (GRU-D) parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.mean_values = mean_values\n",
    "        \n",
    "        self.gamma_x = nn.Linear(input_size, input_size)\n",
    "        \n",
    "        # GRU gate parameters (temporal part) with combined input, hidden state, and mask\n",
    "        self.zl = nn.Linear(input_size + hidden_size + input_size, hidden_size)  # Update gate\n",
    "        self.rl = nn.Linear(input_size + hidden_size + input_size, hidden_size)  # Reset gate\n",
    "        self.hl = nn.Linear(input_size + hidden_size + input_size, hidden_size)  # Candidate hidden state\n",
    "\n",
    "        # Embedding or processing static data\n",
    "        self.static_fc = nn.Linear(static_input_size, static_embedding_size)\n",
    "\n",
    "        # Attention mechanism based on static data\n",
    "        self.attention = nn.Linear(static_embedding_size, hidden_size)\n",
    "\n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, m, delta, static_data):\n",
    "        \"\"\"\n",
    "        Forward pass of GRU-D with static data attention.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Temporal input data [batch_size, seq_len, input_size].\n",
    "            m (torch.Tensor): Masking vector [batch_size, seq_len, input_size].\n",
    "            delta (torch.Tensor): Time intervals [batch_size, seq_len, input_size].\n",
    "            static_data (torch.Tensor): Static data input [batch_size, static_input_size].\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Process static data to generate attention weights\n",
    "        static_embed = torch.relu(self.static_fc(static_data))  # [batch_size, static_embedding_size]\n",
    "        \n",
    "        # Generate attention weights from static data\n",
    "        attention_weights = torch.sigmoid(self.attention(static_embed))  # [batch_size, hidden_size]\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        h = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # Corrected input decay mechanism\n",
    "            gamma_x_t = torch.exp(-F.relu(self.gamma_x(delta[:, t, :])))\n",
    "            x_t_hat = m[:, t, :] * x[:, t, :] + (1 - m[:, t, :]) * (gamma_x_t * self.mean_values + (1 - gamma_x_t) * x[:, t, :])\n",
    "            \n",
    "            # Concatenate input, hidden state, and mask\n",
    "            combined = torch.cat([x_t_hat, h, m[:, t, :]], dim=-1)\n",
    "\n",
    "            # GRU gates\n",
    "            r_t = torch.sigmoid(self.rl(combined))  # Reset gate\n",
    "            z_t = torch.sigmoid(self.zl(combined))  # Update gate\n",
    "            h_tilde = torch.tanh(self.hl(torch.cat([x_t_hat, r_t * h, m[:, t, :]], dim=-1)))  # Candidate hidden state\n",
    "\n",
    "            # Update hidden state\n",
    "            h = (1 - z_t) * h + z_t * h_tilde\n",
    "\n",
    "        # Apply attention weights to the final hidden state\n",
    "        h_weighted = attention_weights * h  # [batch_size, hidden_size]\n",
    "\n",
    "        # Final output\n",
    "        output = self.fc(h_weighted)\n",
    "\n",
    "        # Sigmoid for binary classification\n",
    "        output = torch.sigmoid(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Training Parameters\n",
    "input_size = X_temporal_keras.shape[2]  # Temporal input features\n",
    "hidden_size = 64  # GRU hidden state size\n",
    "output_size = 1  # Binary classification\n",
    "static_input_size = X_static_keras.shape[1]  # Static input features\n",
    "static_embedding_size = 16  # Embedding size for static features\n",
    "mean_values = torch.tensor(np.nanmean(X_temporal_keras, axis=(0, 1)), dtype=torch.float32)  # Temporal feature means\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x = torch.tensor(X_temporal_keras, dtype=torch.float32)  # Temporal input\n",
    "mask = torch.tensor(X_temporal_mask_keras, dtype=torch.float32)  # Masking vector\n",
    "delta = torch.tensor(X_temporal_delta_keras, dtype=torch.float32)  # Time intervals\n",
    "static_data = torch.tensor(X_static_keras, dtype=torch.float32)  # Static input\n",
    "y = torch.tensor(y_keras, dtype=torch.float32)  # Binary labels\n",
    "\n",
    "# Initialize the model\n",
    "model = GRUDWithStaticAttention(input_size, hidden_size, output_size, mean_values, static_input_size, static_embedding_size)\n",
    "\n",
    "def cross_validate_model_with_auc(model_class, x, mask, delta, static_data, y, n_folds=10, n_epochs=100, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Perform 10-fold cross-validation for the GRUDWithStaticAttention model with ROC-AUC.\n",
    "    \n",
    "    Args:\n",
    "        model_class: The class of the model to instantiate.\n",
    "        x (torch.Tensor): Temporal input data [batch_size, seq_len, input_size].\n",
    "        mask (torch.Tensor): Masking vector [batch_size, seq_len, input_size].\n",
    "        delta (torch.Tensor): Time intervals [batch_size, seq_len, input_size].\n",
    "        static_data (torch.Tensor): Static input data [batch_size, static_input_size].\n",
    "        y (torch.Tensor): Binary labels for classification [batch_size, 1].\n",
    "        n_folds (int): Number of folds for cross-validation.\n",
    "        n_epochs (int): Number of epochs for training.\n",
    "        learning_rate (float): Learning rate for optimizer.\n",
    "    \n",
    "    Returns:\n",
    "        mean_precision, mean_recall, mean_f1, mean_roc_auc: Mean metrics across folds.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(x)):\n",
    "        print(f\"Fold {fold + 1}/{n_folds}\")\n",
    "\n",
    "        # Split data into train and test sets\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        mask_train, mask_test = mask[train_index], mask[test_index]\n",
    "        delta_train, delta_test = delta[train_index], delta[test_index]\n",
    "        static_train, static_test = static_data[train_index], static_data[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Initialize the model\n",
    "        model = model_class(\n",
    "            input_size=x.shape[2],\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=output_size,\n",
    "            mean_values=mean_values,\n",
    "            static_input_size=static_input_size,\n",
    "            static_embedding_size=static_embedding_size,\n",
    "        )\n",
    "\n",
    "        # Binary Cross-Entropy Loss and Adam Optimizer\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(n_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(x_train, mask_train, delta_train, static_train)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs.squeeze(), y_train.squeeze())\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate the model on test set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(x_test, mask_test, delta_test, static_test).squeeze()\n",
    "            probabilities = outputs.cpu().numpy()\n",
    "            predictions = (probabilities > 0.906).astype(int)\n",
    "            y_test_numpy = y_test.int().cpu().numpy()\n",
    "\n",
    "            # Compute evaluation metrics\n",
    "            precision = precision_score(y_test_numpy, predictions, zero_division=0)\n",
    "            recall = recall_score(y_test_numpy, predictions, zero_division=0)\n",
    "            f1 = f1_score(y_test_numpy, predictions, zero_division=0)\n",
    "            roc_auc = roc_auc_score(y_test_numpy, probabilities)\n",
    "\n",
    "            fold_results.append({\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1_score\": f1,\n",
    "                \"roc_auc\": roc_auc\n",
    "            })\n",
    "\n",
    "            print(f\"Fold {fold + 1} - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    # Compute mean metrics across all folds\n",
    "    mean_precision = np.mean([result['precision'] for result in fold_results])\n",
    "    mean_recall = np.mean([result['recall'] for result in fold_results])\n",
    "    mean_f1 = np.mean([result['f1_score'] for result in fold_results])\n",
    "    mean_roc_auc = np.mean([result['roc_auc'] for result in fold_results])\n",
    "\n",
    "    return mean_precision, mean_recall, mean_f1, mean_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7efc90cb-b93e-4958-95ad-7fa8caaa14eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\yipeng_wei\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:188: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step \n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "Fold 1/10\n",
      "Fold 1 - Precision: 0.9497, Recall: 0.7558, F1 Score: 0.8417, ROC-AUC: 0.7613\n",
      "Fold 2/10\n",
      "Fold 2 - Precision: 0.9449, Recall: 0.7376, F1 Score: 0.8285, ROC-AUC: 0.7386\n",
      "Fold 3/10\n",
      "Fold 3 - Precision: 0.9337, Recall: 0.7636, F1 Score: 0.8401, ROC-AUC: 0.6804\n",
      "Fold 4/10\n",
      "Fold 4 - Precision: 0.9515, Recall: 0.7432, F1 Score: 0.8345, ROC-AUC: 0.7138\n",
      "Fold 5/10\n",
      "Fold 5 - Precision: 0.9557, Recall: 0.7809, F1 Score: 0.8595, ROC-AUC: 0.7621\n",
      "Fold 6/10\n",
      "Fold 6 - Precision: 0.9529, Recall: 0.7197, F1 Score: 0.8200, ROC-AUC: 0.7230\n",
      "Fold 7/10\n",
      "Fold 7 - Precision: 0.9539, Recall: 0.7013, F1 Score: 0.8083, ROC-AUC: 0.7167\n",
      "Fold 8/10\n",
      "Fold 8 - Precision: 0.9550, Recall: 0.7747, F1 Score: 0.8555, ROC-AUC: 0.8052\n",
      "Fold 9/10\n",
      "Fold 9 - Precision: 0.9543, Recall: 0.7489, F1 Score: 0.8392, ROC-AUC: 0.7471\n",
      "Fold 10/10\n",
      "Fold 10 - Precision: 0.9477, Recall: 0.7382, F1 Score: 0.8299, ROC-AUC: 0.7658\n",
      "          Model  Precision  Recall  F1 Score  ROC-AUC\n",
      "0    PredictPTB      0.947   0.691     0.798    0.719\n",
      "1        RETAIN      0.950   0.714     0.815    0.723\n",
      "2           GRU      0.950   0.682     0.793    0.721\n",
      "3  GRU-D-Static      0.950   0.746     0.836    0.741\n"
     ]
    }
   ],
   "source": [
    "# Prepare lists to store the results\n",
    "models = [\"PredictPTB\", \"RETAIN\", \"GRU\",\"GRU-D-Static\"]\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []  # List for F1 scores\n",
    "roc_aucs = []  # List for ROC-AUC scores\n",
    "\n",
    "# Function to append results from the evaluate_model function\n",
    "def append_results(model_name, precision, recall, f1, roc_auc):\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    roc_aucs.append(roc_auc)\n",
    "\n",
    "\n",
    "# Evaluate PredictPTB, RETAIN, and GRU (3D input)\n",
    "precision, recall, f1, roc_auc = evaluate_model(build_predict_ptb, X_keras, y_keras, model_type=\"keras\")\n",
    "append_results(\"PredictPTB\", precision, recall, f1, roc_auc)\n",
    "\n",
    "precision, recall, f1, roc_auc = evaluate_model(build_retain, X_keras, y_keras, model_type=\"keras\")\n",
    "append_results(\"RETAIN\", precision, recall, f1, roc_auc)\n",
    "\n",
    "precision, recall, f1, roc_auc = evaluate_model(build_gru, X_keras, y_keras, model_type=\"keras\")\n",
    "append_results(\"GRU\", precision, recall, f1, roc_auc)\n",
    "\n",
    "# Evaluate GRU-D-Static (3D input)\n",
    "precision, recall, f1, roc_auc = cross_validate_model_with_auc(\n",
    "    GRUDWithStaticAttention,\n",
    "    x,\n",
    "    mask,\n",
    "    delta,\n",
    "    static_data,\n",
    "    y,\n",
    "    n_folds=10,\n",
    "    n_epochs=100,\n",
    "    learning_rate=0.015\n",
    ")\n",
    "append_results(\"GRU-D-Static\", precision, recall, f1, roc_auc)\n",
    "\n",
    "# Create a summary DataFrame for GRU-D-Static\n",
    "results_summary = pd.DataFrame({\n",
    "    \"Model\": models,\n",
    "    \"Precision\": precisions,\n",
    "    \"Recall\": recalls,\n",
    "    \"F1 Score\": f1_scores,\n",
    "    \"ROC-AUC\": roc_aucs\n",
    "})\n",
    "\n",
    "# Round the numerical values to 3 decimal places\n",
    "results_summary = results_summary.round(3)\n",
    "\n",
    "# Display the summary table\n",
    "print(results_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
